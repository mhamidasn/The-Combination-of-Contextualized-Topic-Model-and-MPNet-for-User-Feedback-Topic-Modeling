{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import octis\n",
    "from octis.optimization.optimizer import Optimizer\n",
    "from skopt.space.space import Real, Categorical, Integer\n",
    "from octis.models.HDP import HDP\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class HDP(AbstractModel):\n",
      "\n",
      "    id2word = None\n",
      "    id_corpus = None\n",
      "    use_partitions = True\n",
      "    update_with_test = False\n",
      "\n",
      "    def __init__(self, max_chunks=None, max_time=None, chunksize=256, kappa=1.0, tau=64.0, K=15, T=150, alpha=1,\n",
      "                 gamma=1, eta=0.01, scale=1.0, var_converge=0.0001):\n",
      "        \"\"\"\n",
      "        Initialize HDP model\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        max_chunks (int, optional) – Upper bound on how many chunks to process.\n",
      "        It wraps around corpus beginning in another corpus pass,\n",
      "        if there are not enough chunks in the corpus.\n",
      "\n",
      "        max_time (int, optional) – Upper bound on time (in seconds)\n",
      "        for which model will be trained.\n",
      "\n",
      "        chunksize (int, optional) – Number of documents in one chuck.\n",
      "\n",
      "        kappa (float,optional) – Learning parameter which acts as exponential\n",
      "        decay factor to influence extent of learning from each batch.\n",
      "\n",
      "        tau (float, optional) – Learning parameter which down-weights\n",
      "        early iterations of documents.\n",
      "\n",
      "        K (int, optional) – Second level truncation level\n",
      "\n",
      "        T (int, optional) – Top level truncation level\n",
      "\n",
      "        alpha (int, optional) – Second level concentration\n",
      "\n",
      "        gamma (int, optional) – First level concentration\n",
      "\n",
      "        eta (float, optional) – The topic Dirichlet\n",
      "\n",
      "        scale (float, optional) – Weights information from the\n",
      "        mini-chunk of corpus to calculate rhot.\n",
      "\n",
      "        var_converge (float, optional) – Lower bound on the right side of\n",
      "        convergence. Used when updating variational parameters\n",
      "        for a single document.\n",
      "        \"\"\"\n",
      "        super().__init__()\n",
      "        self.hyperparameters[\"max_chunks\"] = max_chunks\n",
      "        self.hyperparameters[\"max_time\"] = max_time\n",
      "        self.hyperparameters[\"chunksize\"] = chunksize\n",
      "        self.hyperparameters[\"kappa\"] = kappa\n",
      "        self.hyperparameters[\"tau\"] = tau\n",
      "        self.hyperparameters[\"K\"] = K\n",
      "        self.hyperparameters[\"T\"] = T\n",
      "        self.hyperparameters[\"alpha\"] = alpha\n",
      "        self.hyperparameters[\"gamma\"] = gamma\n",
      "        self.hyperparameters[\"eta\"] = eta\n",
      "        self.hyperparameters[\"scale\"] = scale\n",
      "        self.hyperparameters[\"var_converge\"] = var_converge\n",
      "\n",
      "    def info(self):\n",
      "        \"\"\"\n",
      "        Returns model informations\n",
      "        \"\"\"\n",
      "        return {\n",
      "            \"citation\": citations.models_HDP,\n",
      "            \"name\": \"HDP, Hierarchical Dirichlet Process\"\n",
      "        }\n",
      "\n",
      "    def hyperparameters_info(self):\n",
      "        \"\"\"\n",
      "        Returns hyperparameters informations\n",
      "        \"\"\"\n",
      "        return defaults.HDP_hyperparameters_info\n",
      "\n",
      "    def partitioning(self, use_partitions, update_with_test=False):\n",
      "        \"\"\"\n",
      "        Handle the partitioning system to use and reset the model to perform\n",
      "        new evaluations\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        use_partitions: True if train/set partitioning is needed, False\n",
      "                        otherwise\n",
      "        update_with_test: True if the model should be updated with the test set,\n",
      "                          False otherwise\n",
      "        \"\"\"\n",
      "        self.use_partitions = use_partitions\n",
      "        self.update_with_test = update_with_test\n",
      "        self.id2word = None\n",
      "        self.id_corpus = None\n",
      "\n",
      "    def train_model(self, dataset, hyperparameters={}, topics=10):\n",
      "        \"\"\"\n",
      "        Train the model and return output\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        dataset : dataset to use to build the model\n",
      "        hyperparameters : hyperparameters to build the model\n",
      "        topics : if greather than 0 returns the top k most significant\n",
      "                 words for each topic in the output\n",
      "                 Default True\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        result : dictionary with up to 3 entries,\n",
      "                 'topics', 'topic-word-matrix' and\n",
      "                 'topic-document-matrix'\n",
      "        \"\"\"\n",
      "        partition = []\n",
      "        if self.use_partitions:\n",
      "            partition = dataset.get_partitioned_corpus()\n",
      "        else:\n",
      "            partition = [dataset.get_corpus(), []]\n",
      "\n",
      "        if self.id2word is None:\n",
      "            self.id2word = corpora.Dictionary(dataset.get_corpus())\n",
      "\n",
      "        if self.id_corpus is None:\n",
      "            self.id_corpus = [self.id2word.doc2bow(\n",
      "                document) for document in partition[0]]\n",
      "\n",
      "        hyperparameters[\"corpus\"] = self.id_corpus\n",
      "        hyperparameters[\"id2word\"] = self.id2word\n",
      "        self.hyperparameters.update(hyperparameters)\n",
      "\n",
      "        self.trained_model = hdpmodel.HdpModel(**self.hyperparameters)\n",
      "\n",
      "        result = dict()\n",
      "        result[\"topic-word-matrix\"] = self.trained_model.get_topics()\n",
      "\n",
      "        if topics > 0:\n",
      "            topics_output = []\n",
      "            for topic in result[\"topic-word-matrix\"]:\n",
      "                top_k = np.argsort(topic)[-topics:]\n",
      "                top_k_words = list(reversed([self.id2word[i] for i in top_k]))\n",
      "                topics_output.append(top_k_words)\n",
      "            result[\"topics\"] = topics_output\n",
      "\n",
      "        result[\"topic-document-matrix\"] = self._get_topic_document_matrix()\n",
      "        if self.use_partitions:\n",
      "            new_corpus = [self.id2word.doc2bow(\n",
      "                document) for document in partition[1]]\n",
      "            if self.update_with_test:\n",
      "                self.trained_model.update(new_corpus)\n",
      "                self.id_corpus.extend(new_corpus)\n",
      "\n",
      "                result[\"test-topic-word-matrix\"] = self.trained_model.get_topics()\n",
      "\n",
      "                if topics > 0:\n",
      "                    topics_output = []\n",
      "                    for topic in result[\"test-topic-word-matrix\"]:\n",
      "                        top_k = np.argsort(topic)[-topics:]\n",
      "                        top_k_words = list(\n",
      "                            reversed([self.id2word[i] for i in top_k]))\n",
      "                        topics_output.append(top_k_words)\n",
      "                    result[\"test-topics\"] = topics_output\n",
      "\n",
      "                result[\"test-topic-document-matrix\"] = self._get_topic_document_matrix()\n",
      "\n",
      "            else:\n",
      "                test_document_topic_matrix = []\n",
      "                for document in new_corpus:\n",
      "\n",
      "                    document_topics_tuples = self.trained_model[document]\n",
      "                    document_topics = np.zeros(\n",
      "                        len(self.trained_model.get_topics()))\n",
      "                    for single_tuple in document_topics_tuples:\n",
      "                        document_topics[single_tuple[0]] = single_tuple[1]\n",
      "\n",
      "                    test_document_topic_matrix.append(document_topics)\n",
      "\n",
      "                result[\"test-topic-document-matrix\"] = np.array(\n",
      "                    test_document_topic_matrix).transpose()\n",
      "\n",
      "        return result\n",
      "\n",
      "    def _get_topics_words(self, topics):\n",
      "        \"\"\"\n",
      "        Return the most significative words for each topic.\n",
      "        \"\"\"\n",
      "        topic_terms = []\n",
      "        for i in range(len(self.trained_model.get_topics())):\n",
      "            topic_terms.append(self.trained_model.show_topic(\n",
      "                i,\n",
      "                topics,\n",
      "                False,\n",
      "                True\n",
      "            ))\n",
      "        return topic_terms\n",
      "\n",
      "    def _get_topic_document_matrix(self):\n",
      "        \"\"\"\n",
      "        Return the topic representation of the\n",
      "        corpus\n",
      "        \"\"\"\n",
      "        doc_topic_tuples = []\n",
      "        for document in self.id_corpus:\n",
      "            doc_topic_tuples.append(self.trained_model[document])\n",
      "\n",
      "        topic_document = np.zeros((\n",
      "            len(self.trained_model.get_topics()),\n",
      "            len(doc_topic_tuples)))\n",
      "\n",
      "        for ndoc in range(len(doc_topic_tuples)):\n",
      "            document = doc_topic_tuples[ndoc]\n",
      "            for topic_tuple in document:\n",
      "                topic_document[topic_tuple[0]][ndoc] = topic_tuple[1]\n",
      "        return topic_document\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(octis.models.HDP.HDP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.load_custom_dataset_from_folder(\"content/corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = Coherence(texts=dataset.get_corpus(), measure='c_v')\n",
    "umass = Coherence(texts=dataset.get_corpus(), measure='u_mass')\n",
    "uci = Coherence(texts=dataset.get_corpus(), measure='c_uci')\n",
    "npmi = Coherence(texts=dataset.get_corpus())\n",
    "topic_diversity = TopicDiversity(topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "model = HDP(T=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using default partitioning choice \n",
    "output = model.train_model(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output['topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app game use get video like update time play good\n",
      "app use video get like game update time good fix\n",
      "app video use get like time good update work even\n",
      "app use get video time update like make fix good\n",
      "app video use like get time update game really work\n",
      "app get video use update like even work fix time\n",
      "app use video get time update like good try make\n"
     ]
    }
   ],
   "source": [
    "for t in output['topics']:\n",
    "  print(\" \".join(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic diversity: 0.22857142857142856\n",
      "coherence CV: 0.3302866233704863\n",
      "coherence NPMI: -0.018825262517980217\n",
      "coherence UCI: -0.12049337193272949\n"
     ]
    }
   ],
   "source": [
    "topic_diversity_score = topic_diversity.score(output)\n",
    "cv_score = cv.score(output)\n",
    "npmi_score = npmi.score(output)\n",
    "uci_score = uci.score(output)\n",
    "\n",
    "print(f\"topic diversity: {topic_diversity_score}\")\n",
    "print(f\"coherence CV: {cv_score}\")\n",
    "print(f\"coherence NPMI: {npmi_score}\")\n",
    "print(f\"coherence UCI: {uci_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
