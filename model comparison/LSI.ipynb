{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import octis\n",
    "from octis.optimization.optimizer import Optimizer\n",
    "from skopt.space.space import Real, Categorical, Integer\n",
    "from octis.models.LSI import LSI\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class LSI(AbstractModel):\n",
      "\n",
      "    id2word = None\n",
      "    id_corpus = None\n",
      "    hyperparameters = {}\n",
      "    use_partitions = True\n",
      "    update_with_test = False\n",
      "\n",
      "    def __init__(self, num_topics=200, chunksize=20000, decay=1.0,\n",
      "                 distributed=False, onepass=True, power_iters=2,\n",
      "                 extra_samples=100):\n",
      "        \"\"\"\n",
      "        Initialize LSI model\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        num_topics (int, optional) – Number of requested factors\n",
      "\n",
      "        chunksize (int, optional) – Number of documents to be used in each\n",
      "        training chunk.\n",
      "\n",
      "        decay (float, optional) – Weight of existing observations relatively\n",
      "        to new ones.\n",
      "\n",
      "        distributed (bool, optional) – If True - distributed mode (parallel\n",
      "        execution on several machines) will be used.\n",
      "\n",
      "        onepass (bool, optional) – Whether the one-pass algorithm should be\n",
      "        used for training. Pass False to force a multi-pass stochastic\n",
      "        algorithm.\n",
      "\n",
      "        power_iters (int, optional) – Number of power iteration steps to be\n",
      "        used. Increasing the number of power iterations improves accuracy,\n",
      "        but lowers performance\n",
      "\n",
      "        extra_samples (int, optional) – Extra samples to be used besides the\n",
      "        rank k. Can improve accuracy.\n",
      "        \"\"\"\n",
      "        self.hyperparameters[\"num_topics\"] = num_topics\n",
      "        self.hyperparameters[\"chunksize\"] = chunksize\n",
      "        self.hyperparameters[\"decay\"] = decay\n",
      "        self.hyperparameters[\"distributed\"] = distributed\n",
      "        self.hyperparameters[\"onepass\"] = onepass\n",
      "        self.hyperparameters[\"power_iters\"] = power_iters\n",
      "        self.hyperparameters[\"extra_samples\"] = extra_samples\n",
      "\n",
      "    def info(self):\n",
      "        \"\"\"\n",
      "        Returns model informations\n",
      "        \"\"\"\n",
      "        return {\n",
      "            \"name\": \"LSI, Latent Semantic Indexing\"\n",
      "        }\n",
      "\n",
      "    def hyperparameters_info(self):\n",
      "        \"\"\"\n",
      "        Returns hyperparameters informations\n",
      "        \"\"\"\n",
      "        return defaults.LSI_hyperparameters_info\n",
      "\n",
      "    def partitioning(self, use_partitions, update_with_test=False):\n",
      "        \"\"\"\n",
      "        Handle the partitioning system to use and reset the model to perform\n",
      "        new evaluations\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        use_partitions: True if train/set partitioning is needed, False\n",
      "                        otherwise\n",
      "        update_with_test: True if the model should be updated with the test set,\n",
      "                          False otherwise\n",
      "        \"\"\"\n",
      "        self.use_partitions = use_partitions\n",
      "        self.update_with_test = update_with_test\n",
      "        self.id2word = None\n",
      "        self.id_corpus = None\n",
      "\n",
      "    def train_model(self, dataset, hyperparameters={}, top_words=10):\n",
      "        \"\"\"\n",
      "        Train the model and return output\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        dataset : dataset to use to build the model\n",
      "        hyperparameters : hyperparameters to build the model\n",
      "        top_words : if greather than 0 returns the most significant words\n",
      "                 for each topic in the output\n",
      "                 Default True\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        result : dictionary with up to 3 entries,\n",
      "                 'topics', 'topic-word-matrix' and\n",
      "                 'topic-document-matrix'\n",
      "        \"\"\"\n",
      "        partition = []\n",
      "        if self.use_partitions:\n",
      "            partition = dataset.get_partitioned_corpus(use_validation=False)\n",
      "        else:\n",
      "            partition = [dataset.get_corpus(), []]\n",
      "\n",
      "        if self.id2word == None:\n",
      "            self.id2word = corpora.Dictionary(dataset.get_corpus())\n",
      "\n",
      "        if self.id_corpus == None:\n",
      "            self.id_corpus = [self.id2word.doc2bow(\n",
      "                document) for document in partition[0]]\n",
      "\n",
      "        hyperparameters[\"corpus\"] = self.id_corpus\n",
      "        hyperparameters[\"id2word\"] = self.id2word\n",
      "        self.hyperparameters.update(hyperparameters)\n",
      "\n",
      "        self.trained_model = lsimodel.LsiModel(**self.hyperparameters)\n",
      "\n",
      "        result = {}\n",
      "\n",
      "        result[\"topic-word-matrix\"] = self._get_topic_word_matrix()\n",
      "\n",
      "        if top_words > 0:\n",
      "            topics_output = []\n",
      "            for topic in result[\"topic-word-matrix\"]:\n",
      "                top_k = np.argsort(topic)[-top_words:]\n",
      "                top_k_words = list(reversed([self.id2word[i] for i in top_k]))\n",
      "                topics_output.append(top_k_words)\n",
      "            result[\"topics\"] = topics_output\n",
      "\n",
      "        result[\"topic-document-matrix\"] = self._get_topic_document_matrix()\n",
      "\n",
      "        if self.use_partitions:\n",
      "            new_corpus = [self.id2word.doc2bow(\n",
      "                document) for document in partition[1]]\n",
      "            if self.update_with_test:\n",
      "                self.trained_model.add_documents(new_corpus)\n",
      "                self.id_corpus.extend(new_corpus)\n",
      "\n",
      "                result[\"test-topic-word-matrix\"] = (\n",
      "                    self._get_topic_word_matrix())\n",
      "\n",
      "                if top_words > 0:\n",
      "                    topics_output = []\n",
      "                    for topic in result[\"test-topic-word-matrix\"]:\n",
      "                        top_k = np.argsort(topic)[-top_words:]\n",
      "                        top_k_words = list(\n",
      "                            reversed([self.id2word[i] for i in top_k]))\n",
      "                        topics_output.append(top_k_words)\n",
      "                    result[\"test-topics\"] = topics_output\n",
      "\n",
      "                result[\"test-topic-document-matrix\"] = (\n",
      "                    self._get_topic_document_matrix())\n",
      "\n",
      "            else:\n",
      "                test_document_topic_matrix = []\n",
      "                for document in new_corpus:\n",
      "\n",
      "                    document_topics_tuples = self.trained_model[document]\n",
      "                    document_topics = np.zeros(\n",
      "                        self.hyperparameters[\"num_topics\"])\n",
      "                    for single_tuple in document_topics_tuples:\n",
      "                        document_topics[single_tuple[0]] = single_tuple[1]\n",
      "\n",
      "                    test_document_topic_matrix.append(document_topics)\n",
      "\n",
      "                result[\"test-topic-document-matrix\"] = np.array(\n",
      "                    test_document_topic_matrix).transpose()\n",
      "\n",
      "        return result\n",
      "\n",
      "    def _get_topic_word_matrix(self):\n",
      "        \"\"\"\n",
      "        Return the topic representation of the words\n",
      "        \"\"\"\n",
      "        topic_word_matrix = self.trained_model.get_topics()\n",
      "        normalized = []\n",
      "        for words_w in topic_word_matrix:\n",
      "            minimum = min(words_w)\n",
      "            words = words_w - minimum\n",
      "            normalized.append([float(i)/sum(words) for i in words])\n",
      "        topic_word_matrix = np.array(normalized)\n",
      "        return topic_word_matrix\n",
      "\n",
      "    def _get_topics_words(self, topics):\n",
      "        \"\"\"\n",
      "        Return the most significative words for each topic.\n",
      "        \"\"\"\n",
      "        topic_terms = []\n",
      "        for i in range(self.hyperparameters[\"num_topics\"]):\n",
      "            topic_words_list = []\n",
      "            for word_tuple in self.trained_model.show_topic(i, topics):\n",
      "                topic_words_list.append(word_tuple[0])\n",
      "            topic_terms.append(topic_words_list)\n",
      "        return topic_terms\n",
      "\n",
      "    def _get_topic_document_matrix(self):\n",
      "        \"\"\"\n",
      "        Return the topic representation of the\n",
      "        corpus\n",
      "        \"\"\"\n",
      "        topic_weights = self.trained_model[self.id_corpus]\n",
      "\n",
      "        topic_document = []\n",
      "\n",
      "        for document_topic_weights in topic_weights:\n",
      "\n",
      "            # Find min e max topic_weights values\n",
      "            minimum = document_topic_weights[0][1]\n",
      "            maximum = document_topic_weights[0][1]\n",
      "            for topic in document_topic_weights:\n",
      "                if topic[1] > maximum:\n",
      "                    maximum = topic[1]\n",
      "                if topic[1] < minimum:\n",
      "                    minimum = topic[1]\n",
      "\n",
      "            # For each topic compute normalized weight\n",
      "            # in the form (value-min)/(max-min)\n",
      "            topic_w = []\n",
      "            for topic in document_topic_weights:\n",
      "                topic_w.append((topic[1]-minimum)/(maximum-minimum))\n",
      "            topic_document.append(topic_w)\n",
      "\n",
      "        return np.array(topic_document).transpose()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(octis.models.LSI.LSI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.load_custom_dataset_from_folder(\"content/corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = Coherence(texts=dataset.get_corpus(), measure='c_v')\n",
    "umass = Coherence(texts=dataset.get_corpus(), measure='u_mass')\n",
    "uci = Coherence(texts=dataset.get_corpus(), measure='c_uci')\n",
    "npmi = Coherence(texts=dataset.get_corpus())\n",
    "topic_diversity = TopicDiversity(topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "model = LSI(num_topics=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using default partitioning choice \n",
    "output = model.train_model(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app use get video like update time game even good\n",
      "app use message send open cash work video reinstall notification\n",
      "app game cash money love fun roblox open good great\n",
      "video app game play ad watch good youtube amazing edit\n",
      "song use like ad music playlist want spotify listen play\n",
      "ad get song play time even watch every app listen\n",
      "update song play fix music playlist please work new spotify\n"
     ]
    }
   ],
   "source": [
    "for t in output['topics']:\n",
    "  print(\" \".join(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic diversity: 0.5428571428571428\n",
      "coherence CV: 0.45886019613441015\n",
      "coherence NPMI: 0.02907225588259218\n",
      "coherence UCI: 0.07320196452086689\n"
     ]
    }
   ],
   "source": [
    "topic_diversity_score = topic_diversity.score(output)\n",
    "cv_score = cv.score(output)\n",
    "npmi_score = npmi.score(output)\n",
    "uci_score = uci.score(output)\n",
    "\n",
    "print(f\"topic diversity: {topic_diversity_score}\")\n",
    "print(f\"coherence CV: {cv_score}\")\n",
    "print(f\"coherence NPMI: {npmi_score}\")\n",
    "print(f\"coherence UCI: {uci_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
