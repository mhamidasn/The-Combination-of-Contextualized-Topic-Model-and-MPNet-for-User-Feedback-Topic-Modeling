{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import octis\n",
    "from octis.optimization.optimizer import Optimizer\n",
    "from skopt.space.space import Real, Categorical, Integer\n",
    "from octis.models.LDA import LDA\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class LDA(AbstractModel):\n",
      "\n",
      "    id2word = None\n",
      "    id_corpus = None\n",
      "    use_partitions = True\n",
      "    update_with_test = False\n",
      "\n",
      "    def __init__(\n",
      "        self, num_topics=100, distributed=False, chunksize=2000,\n",
      "        passes=1, update_every=1, alpha=\"symmetric\", eta=None, decay=0.5,\n",
      "        offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001,\n",
      "            random_state=None):\n",
      "        \"\"\"\n",
      "        Initialize LDA model\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        num_topics (int, optional) – The number of requested latent topics to\n",
      "        be extracted from the training corpus.\n",
      "\n",
      "        distributed (bool, optional) – Whether distributed computing should be\n",
      "        used to accelerate training.\n",
      "\n",
      "        chunksize (int, optional) – Number of documents to be used in each\n",
      "        training chunk.\n",
      "\n",
      "        passes (int, optional) – Number of passes through the corpus during\n",
      "        training.\n",
      "\n",
      "        update_every (int, optional) – Number of documents to be iterated\n",
      "        through for each update. Set to 0 for batch learning, > 1 for\n",
      "        online iterative learning.\n",
      "\n",
      "        alpha ({numpy.ndarray, str}, optional) – Can be set to an 1D array of\n",
      "        length equal to the number of expected topics that expresses our\n",
      "        a-priori belief for the each topics’ probability. Alternatively\n",
      "        default prior selecting strategies can be employed by supplying\n",
      "        a string:\n",
      "\n",
      "            ’asymmetric’: Uses a fixed normalized asymmetric prior of\n",
      "            1.0 / topicno.\n",
      "\n",
      "            ’auto’: Learns an asymmetric prior from the corpus\n",
      "            (not available if distributed==True).\n",
      "\n",
      "        eta ({float, np.array, str}, optional) – A-priori belief on word\n",
      "        probability, this can be:\n",
      "\n",
      "            scalar for a symmetric prior over topic/word probability,\n",
      "\n",
      "            vector of length num_words to denote an asymmetric user defined\n",
      "            probability for each word,\n",
      "\n",
      "            matrix of shape (num_topics, num_words) to assign a probability\n",
      "            for each word-topic combination,\n",
      "\n",
      "            the string ‘auto’ to learn the asymmetric prior from the data.\n",
      "\n",
      "        decay (float, optional) – A number between (0.5, 1] to weight what\n",
      "        percentage of the previous lambda value is forgotten when each new\n",
      "        document is examined.\n",
      "\n",
      "        offset (float, optional) – Hyper-parameter that controls how much\n",
      "        we will slow down the first steps the first few iterations.\n",
      "\n",
      "        eval_every (int, optional) – Log perplexity is estimated every\n",
      "        that many updates. Setting this to one slows down training by ~2x.\n",
      "\n",
      "        iterations (int, optional) – Maximum number of iterations through the\n",
      "        corpus when inferring the topic distribution of a corpus.\n",
      "\n",
      "        gamma_threshold (float, optional) – Minimum change in the value of the\n",
      "        gamma parameters to continue iterating.\n",
      "\n",
      "        random_state ({np.random.RandomState, int}, optional) – Either a\n",
      "        randomState object or a seed to generate one.s\n",
      "        Useful for reproducibility.\n",
      "\n",
      "\n",
      "        \"\"\"\n",
      "        super().__init__()\n",
      "        self.hyperparameters = dict()\n",
      "        self.hyperparameters[\"num_topics\"] = num_topics\n",
      "        self.hyperparameters[\"distributed\"] = distributed\n",
      "        self.hyperparameters[\"chunksize\"] = chunksize\n",
      "        self.hyperparameters[\"passes\"] = passes\n",
      "        self.hyperparameters[\"update_every\"] = update_every\n",
      "        self.hyperparameters[\"alpha\"] = alpha\n",
      "        self.hyperparameters[\"eta\"] = eta\n",
      "        self.hyperparameters[\"decay\"] = decay\n",
      "        self.hyperparameters[\"offset\"] = offset\n",
      "        self.hyperparameters[\"eval_every\"] = eval_every\n",
      "        self.hyperparameters[\"iterations\"] = iterations\n",
      "        self.hyperparameters[\"gamma_threshold\"] = gamma_threshold\n",
      "        self.hyperparameters[\"random_state\"] = random_state\n",
      "\n",
      "    def info(self):\n",
      "        \"\"\"\n",
      "        Returns model informations\n",
      "        \"\"\"\n",
      "        return {\n",
      "            \"citation\": citations.models_LDA,\n",
      "            \"name\": \"LDA, Latent Dirichlet Allocation\"\n",
      "        }\n",
      "\n",
      "    def hyperparameters_info(self):\n",
      "        \"\"\"\n",
      "        Returns hyperparameters informations\n",
      "        \"\"\"\n",
      "        return defaults.LDA_hyperparameters_info\n",
      "\n",
      "    def set_hyperparameters(self, **kwargs):\n",
      "        \"\"\"\n",
      "        Set model hyperparameters\n",
      "        \"\"\"\n",
      "        super().set_hyperparameters(**kwargs)\n",
      "        # Allow alpha to be a float in case of symmetric alpha\n",
      "        if \"alpha\" in kwargs:\n",
      "            if isinstance(kwargs[\"alpha\"], float):\n",
      "                self.hyperparameters[\"alpha\"] = [\n",
      "                    kwargs[\"alpha\"]\n",
      "                ] * self.hyperparameters[\"num_topics\"]\n",
      "\n",
      "    def partitioning(self, use_partitions, update_with_test=False):\n",
      "        \"\"\"\n",
      "        Handle the partitioning system to use and reset the model to perform\n",
      "        new evaluations\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        use_partitions: True if train/set partitioning is needed, False\n",
      "                        otherwise\n",
      "        update_with_test: True if the model should be updated with the test set,\n",
      "                          False otherwise\n",
      "        \"\"\"\n",
      "        self.use_partitions = use_partitions\n",
      "        self.update_with_test = update_with_test\n",
      "        self.id2word = None\n",
      "        self.id_corpus = None\n",
      "\n",
      "    def train_model(self, dataset, hyperparams=None, top_words=10):\n",
      "        \"\"\"\n",
      "        Train the model and return output\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        dataset : dataset to use to build the model\n",
      "        hyperparams : hyperparameters to build the model\n",
      "        top_words : if greater than 0 returns the most significant words for\n",
      "                    each topic in the output (Default True)\n",
      "        Returns\n",
      "        -------\n",
      "        result : dictionary with up to 3 entries,\n",
      "                 'topics', 'topic-word-matrix' and\n",
      "                 'topic-document-matrix'\n",
      "        \"\"\"\n",
      "        if hyperparams is None:\n",
      "            hyperparams = {}\n",
      "\n",
      "        if self.use_partitions:\n",
      "            train_corpus, test_corpus = dataset.get_partitioned_corpus(\n",
      "                use_validation=False)\n",
      "        else:\n",
      "            train_corpus = dataset.get_corpus()\n",
      "\n",
      "        if self.id2word is None:\n",
      "            self.id2word = corpora.Dictionary(dataset.get_corpus())\n",
      "\n",
      "        if self.id_corpus is None:\n",
      "            self.id_corpus = [self.id2word.doc2bow(document)\n",
      "                              for document in train_corpus]\n",
      "\n",
      "        if \"num_topics\" not in hyperparams:\n",
      "            hyperparams[\"num_topics\"] = self.hyperparameters[\"num_topics\"]\n",
      "\n",
      "        # Allow alpha to be a float in case of symmetric alpha\n",
      "        if \"alpha\" in hyperparams:\n",
      "            if isinstance(hyperparams[\"alpha\"], float):\n",
      "                hyperparams[\"alpha\"] = [\n",
      "                    hyperparams[\"alpha\"]\n",
      "                ] * hyperparams[\"num_topics\"]\n",
      "\n",
      "        hyperparams[\"corpus\"] = self.id_corpus\n",
      "        hyperparams[\"id2word\"] = self.id2word\n",
      "        self.hyperparameters.update(hyperparams)\n",
      "\n",
      "        self.trained_model = ldamodel.LdaModel(**self.hyperparameters)\n",
      "\n",
      "        result = {}\n",
      "\n",
      "        result[\"topic-word-matrix\"] = self.trained_model.get_topics()\n",
      "\n",
      "        if top_words > 0:\n",
      "            topics_output = []\n",
      "            for topic in result[\"topic-word-matrix\"]:\n",
      "                top_k = np.argsort(topic)[-top_words:]\n",
      "                top_k_words = list(reversed([self.id2word[i] for i in top_k]))\n",
      "                topics_output.append(top_k_words)\n",
      "            result[\"topics\"] = topics_output\n",
      "\n",
      "        result[\"topic-document-matrix\"] = self._get_topic_document_matrix()\n",
      "\n",
      "        if self.use_partitions:\n",
      "            new_corpus = [self.id2word.doc2bow(\n",
      "                document) for document in test_corpus]\n",
      "            if self.update_with_test:\n",
      "                self.trained_model.update(new_corpus)\n",
      "                self.id_corpus.extend(new_corpus)\n",
      "\n",
      "                result[\"test-topic-word-matrix\"] = (\n",
      "                    self.trained_model.get_topics())\n",
      "\n",
      "                if top_words > 0:\n",
      "                    topics_output = []\n",
      "                    for topic in result[\"test-topic-word-matrix\"]:\n",
      "                        top_k = np.argsort(topic)[-top_words:]\n",
      "                        top_k_words = list(\n",
      "                            reversed([self.id2word[i] for i in top_k]))\n",
      "                        topics_output.append(top_k_words)\n",
      "                    result[\"test-topics\"] = topics_output\n",
      "\n",
      "                result[\"test-topic-document-matrix\"] = (\n",
      "                    self._get_topic_document_matrix())\n",
      "\n",
      "            else:\n",
      "                test_document_topic_matrix = []\n",
      "                for document in new_corpus:\n",
      "                    document_topics_tuples = self.trained_model[document]\n",
      "                    document_topics = np.zeros(\n",
      "                        self.hyperparameters[\"num_topics\"])\n",
      "                    for single_tuple in document_topics_tuples:\n",
      "                        document_topics[single_tuple[0]] = single_tuple[1]\n",
      "\n",
      "                    test_document_topic_matrix.append(document_topics)\n",
      "                result[\"test-topic-document-matrix\"] = np.array(\n",
      "                    test_document_topic_matrix).transpose()\n",
      "        return result\n",
      "\n",
      "    def _get_topics_words(self, topk):\n",
      "        \"\"\"\n",
      "        Return the most significative words for each topic.\n",
      "        \"\"\"\n",
      "        topic_terms = []\n",
      "        for i in range(self.hyperparameters[\"num_topics\"]):\n",
      "            topic_words_list = []\n",
      "            for word_tuple in self.trained_model.get_topic_terms(i, topk):\n",
      "                topic_words_list.append(self.id2word[word_tuple[0]])\n",
      "            topic_terms.append(topic_words_list)\n",
      "        return topic_terms\n",
      "\n",
      "    def _get_topic_document_matrix(self):\n",
      "        \"\"\"\n",
      "        Return the topic representation of the\n",
      "        corpus\n",
      "        \"\"\"\n",
      "        doc_topic_tuples = []\n",
      "        for document in self.id_corpus:\n",
      "            doc_topic_tuples.append(\n",
      "                self.trained_model.get_document_topics(document,\n",
      "                                                       minimum_probability=0))\n",
      "\n",
      "        topic_document = np.zeros((\n",
      "            self.hyperparameters[\"num_topics\"],\n",
      "            len(doc_topic_tuples)))\n",
      "\n",
      "        for ndoc in range(len(doc_topic_tuples)):\n",
      "            document = doc_topic_tuples[ndoc]\n",
      "            for topic_tuple in document:\n",
      "                topic_document[topic_tuple[0]][ndoc] = topic_tuple[1]\n",
      "        return topic_document\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(octis.models.LDA.LDA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.load_custom_dataset_from_folder(\"content/corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = Coherence(texts=dataset.get_corpus(), measure='c_v')\n",
    "umass = Coherence(texts=dataset.get_corpus(), measure='u_mass')\n",
    "uci = Coherence(texts=dataset.get_corpus(), measure='c_uci')\n",
    "npmi = Coherence(texts=dataset.get_corpus())\n",
    "topic_diversity = TopicDiversity(topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "model = LDA(num_topics=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using default partitioning choice \n",
    "output = model.train_model(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game good roblox like really make play love get add\n",
      "app fix work problem nt try even say update crash\n",
      "game app get use make like good time money great\n",
      "app account use good one download online time can phone\n",
      "screen app go fix back update get time use bug\n",
      "play fix game app please download video character update sometimes\n",
      "playlist video ad app want youtube screen watch like use\n"
     ]
    }
   ],
   "source": [
    "for t in output['topics']:\n",
    "  print(\" \".join(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic diversity: 0.6142857142857143\n",
      "coherence CV: 0.4195004506279526\n",
      "coherence NPMI: 0.009623189261826785\n",
      "coherence UCI: 0.02821437644993594\n"
     ]
    }
   ],
   "source": [
    "topic_diversity_score = topic_diversity.score(output)\n",
    "cv_score = cv.score(output)\n",
    "npmi_score = npmi.score(output)\n",
    "uci_score = uci.score(output)\n",
    "\n",
    "print(f\"topic diversity: {topic_diversity_score}\")\n",
    "print(f\"coherence CV: {cv_score}\")\n",
    "print(f\"coherence NPMI: {npmi_score}\")\n",
    "print(f\"coherence UCI: {uci_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
