{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import octis\n",
    "from octis.optimization.optimizer import Optimizer\n",
    "from skopt.space.space import Real, Categorical, Integer\n",
    "from octis.models.ETM import ETM\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ETM(BaseETM):\n",
      "\n",
      "    def __init__(\n",
      "        self, num_topics=10, num_epochs=100, t_hidden_size=800, rho_size=300,\n",
      "        embedding_size=300, activation='relu', dropout=0.5, lr=0.005,\n",
      "        optimizer='adam', batch_size=128, clip=0.0, wdecay=1.2e-6, bow_norm=1,\n",
      "        device='cpu', train_embeddings=True, embeddings_path=None,\n",
      "            embeddings_type='pickle', binary_embeddings=True,\n",
      "            headerless_embeddings=False, use_partitions=True):\n",
      "        \"\"\"\n",
      "        initialization of ETM\n",
      "\n",
      "        :param embeddings_path: string, path to embeddings file.\n",
      "            Can be a binary file for the 'pickle', 'keyedvectors' and\n",
      "            'word2vec' types or a text file for 'word2vec'.\n",
      "            This parameter is only used if 'train_embeddings' is set to False\n",
      "        :param embeddings_type: string, defines the format of the embeddings\n",
      "            file. Possible values are 'pickle', 'keyedvectors' or 'word2vec'.\n",
      "            If set to 'pickle', you must provide a file created with 'pickle'\n",
      "            containing an array of word embeddings, composed by words and\n",
      "            their respective vectors. If set to 'keyedvectors', you must\n",
      "            provide a file containing a saved gensim.models.KeyedVectors\n",
      "            instance. If set to 'word2vec', you must provide a file with the\n",
      "            original word2vec format. This parameter is only used if\n",
      "            'train_embeddings' is set to False (default 'pickle')\n",
      "        :param binary_embeddings: bool, indicates if the original word2vec\n",
      "            embeddings file is binary or textual. This parameter is only used\n",
      "            if both 'embeddings_type' is set to 'word2vec' and\n",
      "            'train_embeddings' is set to False. Otherwise, it will be ignored\n",
      "            (default True)\n",
      "        :param headerless_embeddings: bool, indicates if the original word2vec\n",
      "            embeddings textual file has a header line in the format\n",
      "            \"<no_of_vectors> <vector_length>\". This parameter is only used if\n",
      "            'embeddings_type' is set to 'word2vec', 'train_embeddings' is set\n",
      "            to False and 'binary_embeddings' is set to False. Otherwise, it\n",
      "            will be ignored (default False)\n",
      "        \"\"\"\n",
      "        super(ETM, self).__init__()\n",
      "        self.hyperparameters = dict()\n",
      "        self.hyperparameters['num_topics'] = int(num_topics)\n",
      "        self.hyperparameters['num_epochs'] = int(num_epochs)\n",
      "        self.hyperparameters['t_hidden_size'] = int(t_hidden_size)\n",
      "        self.hyperparameters['rho_size'] = int(rho_size)\n",
      "        self.hyperparameters['embedding_size'] = int(embedding_size)\n",
      "        self.hyperparameters['activation'] = activation\n",
      "        self.hyperparameters['dropout'] = float(dropout)\n",
      "        self.hyperparameters['lr'] = float(lr)\n",
      "        self.hyperparameters['optimizer'] = optimizer\n",
      "        self.hyperparameters['batch_size'] = int(batch_size)\n",
      "        self.hyperparameters['clip'] = float(clip)\n",
      "        self.hyperparameters['wdecay'] = float(wdecay)\n",
      "        self.hyperparameters['bow_norm'] = int(bow_norm)\n",
      "        self.hyperparameters['train_embeddings'] = bool(train_embeddings)\n",
      "        self.hyperparameters['embeddings_path'] = embeddings_path\n",
      "        assert embeddings_type in ['pickle', 'word2vec', 'keyedvectors'], \\\n",
      "            \"embeddings_type must be 'pickle', 'word2vec' or 'keyedvectors'.\"\n",
      "        self.hyperparameters['embeddings_type'] = embeddings_type\n",
      "        self.hyperparameters['binary_embeddings'] = binary_embeddings\n",
      "        self.hyperparameters['headerless_embeddings'] = headerless_embeddings\n",
      "        self.early_stopping = None\n",
      "        self.device = device\n",
      "        self.test_tokens, self.test_counts = None, None\n",
      "        self.valid_tokens, self.valid_counts = None, None\n",
      "        self.train_tokens, self.train_counts, self.vocab = None, None, None\n",
      "        self.use_partitions = use_partitions\n",
      "        self.model = None\n",
      "        self.optimizer = None\n",
      "        self.embeddings = None\n",
      "\n",
      "    def train_model(\n",
      "            self, dataset, hyperparameters=None, top_words=10,\n",
      "            op_path='checkpoint.pt'):\n",
      "        if hyperparameters is None:\n",
      "            hyperparameters = {}\n",
      "        self.set_model(dataset, hyperparameters)\n",
      "        self.top_words = top_words\n",
      "        self.early_stopping = EarlyStopping(\n",
      "            patience=5, verbose=True, path=op_path)\n",
      "\n",
      "        for epoch in range(0, self.hyperparameters['num_epochs']):\n",
      "            continue_training = self._train_epoch(epoch)\n",
      "            if not continue_training:\n",
      "                break\n",
      "\n",
      "        # load the last checkpoint with the best model\n",
      "        # self.model.load_state_dict(torch.load('etm_checkpoint.pt'))\n",
      "\n",
      "        if self.use_partitions:\n",
      "            result = self.inference()\n",
      "        else:\n",
      "            result = self.get_info()\n",
      "\n",
      "        return result\n",
      "\n",
      "    def set_model(self, dataset, hyperparameters):\n",
      "        if self.use_partitions:\n",
      "            train_data, validation_data, testing_data = (\n",
      "                dataset.get_partitioned_corpus(use_validation=True))\n",
      "\n",
      "            data_corpus_train = [' '.join(i) for i in train_data]\n",
      "            data_corpus_test = [' '.join(i) for i in testing_data]\n",
      "            data_corpus_val = [' '.join(i) for i in validation_data]\n",
      "\n",
      "            vocab = dataset.get_vocabulary()\n",
      "            self.vocab = {i: w for i, w in enumerate(vocab)}\n",
      "            vocab2id = {w: i for i, w in enumerate(vocab)}\n",
      "\n",
      "            (self.train_tokens, self.train_counts, self.test_tokens,\n",
      "             self.test_counts, self.valid_tokens, self.valid_counts\n",
      "             ) = self.preprocess(\n",
      "                vocab2id, data_corpus_train, data_corpus_test, data_corpus_val)\n",
      "\n",
      "        else:\n",
      "            data_corpus = [' '.join(i) for i in dataset.get_corpus()]\n",
      "            vocab = dataset.get_vocabulary()\n",
      "            self.vocab = {i: w for i, w in enumerate(vocab)}\n",
      "            vocab2id = {w: i for i, w in enumerate(vocab)}\n",
      "\n",
      "            self.train_tokens, self.train_counts = self.preprocess(\n",
      "                vocab2id, data_corpus, None)\n",
      "\n",
      "        self.device = torch.device(\n",
      "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "        self.set_default_hyperparameters(hyperparameters)\n",
      "        self.load_embeddings()\n",
      "        # define model and optimizer\n",
      "        self.model = etm.ETM(\n",
      "            num_topics=self.hyperparameters['num_topics'],\n",
      "            vocab_size=len(self.vocab.keys()),\n",
      "            t_hidden_size=int(self.hyperparameters['t_hidden_size']),\n",
      "            rho_size=int(self.hyperparameters['rho_size']),\n",
      "            emb_size=int(self.hyperparameters['embedding_size']),\n",
      "            theta_act=self.hyperparameters['activation'],\n",
      "            embeddings=self.embeddings,\n",
      "            train_embeddings=self.hyperparameters['train_embeddings'],\n",
      "            enc_drop=self.hyperparameters['dropout']).to(self.device)\n",
      "        print('model: {}'.format(self.model))\n",
      "\n",
      "        self.optimizer = self.set_optimizer()\n",
      "\n",
      "    def _train_epoch(self, epoch):\n",
      "        self.data_list = []\n",
      "        self.model.train()\n",
      "        acc_loss = 0\n",
      "        acc_kl_theta_loss = 0\n",
      "        cnt = 0\n",
      "        indices = torch.arange(0, len(self.train_tokens))\n",
      "        indices = torch.split(indices, self.hyperparameters['batch_size'])\n",
      "        for idx, ind in enumerate(indices):\n",
      "            self.optimizer.zero_grad()\n",
      "            self.model.zero_grad()\n",
      "            data_batch = data.get_batch(\n",
      "                self.train_tokens, self.train_counts, ind,\n",
      "                len(self.vocab.keys()), self.device)\n",
      "            sums = data_batch.sum(1).unsqueeze(1)\n",
      "            if self.hyperparameters['bow_norm']:\n",
      "                normalized_data_batch = data_batch / sums\n",
      "            else:\n",
      "                normalized_data_batch = data_batch\n",
      "            recon_loss, kld_theta = self.model(\n",
      "                data_batch, normalized_data_batch)\n",
      "            total_loss = recon_loss + kld_theta\n",
      "            total_loss.backward()\n",
      "            if self.hyperparameters[\"clip\"] > 0:\n",
      "                torch.nn.utils.clip_grad_norm_(self.model.parameters(),\n",
      "                                               self.hyperparameters[\"clip\"])\n",
      "            self.optimizer.step()\n",
      "\n",
      "            acc_loss += torch.sum(recon_loss).item()\n",
      "            acc_kl_theta_loss += torch.sum(kld_theta).item()\n",
      "            cnt += 1\n",
      "            log_interval = 20\n",
      "            if idx % log_interval == 0 and idx > 0:\n",
      "                cur_loss = round(acc_loss / cnt, 2)\n",
      "                cur_kl_theta = round(acc_kl_theta_loss / cnt, 2)\n",
      "                cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
      "\n",
      "                print(\n",
      "                    'Epoch: {} .. batch: {}/{} .. LR: {} .. KL_theta: {} ..'\n",
      "                    ' Rec_loss: {} .. NELBO: {}'.format(\n",
      "                        epoch + 1, idx, len(indices),\n",
      "                        self.optimizer.param_groups[0]['lr'],\n",
      "                        cur_kl_theta, cur_loss, cur_real_loss))\n",
      "\n",
      "            self.data_list.append(normalized_data_batch)\n",
      "\n",
      "        cur_loss = round(acc_loss / cnt, 2)\n",
      "        cur_kl_theta = round(acc_kl_theta_loss / cnt, 2)\n",
      "        cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
      "        print('*' * 100)\n",
      "        print(\n",
      "            'Epoch----->{} .. LR: {} .. KL_theta: {} .. '\n",
      "            'Rec_loss: {} .. NELBO: {}'.format(\n",
      "                epoch + 1, self.optimizer.param_groups[0]['lr'], cur_kl_theta,\n",
      "                cur_loss, cur_real_loss))\n",
      "        print('*' * 100)\n",
      "\n",
      "        # VALIDATION ###\n",
      "        if self.valid_tokens is None:\n",
      "            return True\n",
      "        else:\n",
      "            model = self.model.to(self.device)\n",
      "            model.eval()\n",
      "            with torch.no_grad():\n",
      "                val_acc_loss = 0\n",
      "                val_acc_kl_theta_loss = 0\n",
      "                val_cnt = 0\n",
      "                indices = torch.arange(0, len(self.valid_tokens))\n",
      "                indices = torch.split(\n",
      "                    indices, self.hyperparameters['batch_size'])\n",
      "                for idx, ind in enumerate(indices):\n",
      "                    self.optimizer.zero_grad()\n",
      "                    self.model.zero_grad()\n",
      "                    val_data_batch = data.get_batch(\n",
      "                        self.valid_tokens, self.valid_counts,\n",
      "                        ind, len(self.vocab.keys()), self.device)\n",
      "                    sums = val_data_batch.sum(1).unsqueeze(1)\n",
      "                    if self.hyperparameters['bow_norm']:\n",
      "                        val_normalized_data_batch = val_data_batch / sums\n",
      "                    else:\n",
      "                        val_normalized_data_batch = val_data_batch\n",
      "\n",
      "                    val_recon_loss, val_kld_theta = self.model(\n",
      "                        val_data_batch, val_normalized_data_batch)\n",
      "\n",
      "                    val_acc_loss += torch.sum(val_recon_loss).item()\n",
      "                    val_acc_kl_theta_loss += torch.sum(val_kld_theta).item()\n",
      "                    val_cnt += 1\n",
      "                    val_total_loss = val_recon_loss + val_kld_theta\n",
      "\n",
      "                val_cur_loss = round(val_acc_loss / cnt, 2)\n",
      "                val_cur_kl_theta = round(val_acc_kl_theta_loss / cnt, 2)\n",
      "                val_cur_real_loss = round(val_cur_loss + val_cur_kl_theta, 2)\n",
      "                print('*' * 100)\n",
      "                print(\n",
      "                    'VALIDATION .. LR: {} .. KL_theta: {} .. Rec_loss: {}'\n",
      "                    ' .. NELBO: {}'.format(\n",
      "                        self.optimizer.param_groups[0]['lr'], val_cur_kl_theta,\n",
      "                        val_cur_loss, val_cur_real_loss))\n",
      "                print('*' * 100)\n",
      "                if np.isnan(val_cur_real_loss):\n",
      "                    return False\n",
      "                else:\n",
      "                    self.early_stopping(val_total_loss, model)\n",
      "\n",
      "                    if self.early_stopping.early_stop:\n",
      "                        print(\"Early stopping\")\n",
      "                        return False\n",
      "                    else:\n",
      "                        return True\n",
      "\n",
      "    def get_info(self):\n",
      "        topic_w = []\n",
      "        self.model.eval()\n",
      "        info = {}\n",
      "        with torch.no_grad():\n",
      "            theta, _ = self.model.get_theta(torch.cat(self.data_list))\n",
      "            gammas = self.model.get_beta().cpu().numpy()\n",
      "            for k in range(self.hyperparameters['num_topics']):\n",
      "                if np.isnan(gammas[k]).any():\n",
      "                    # to deal with nan matrices\n",
      "                    topic_w = None\n",
      "                    break\n",
      "                else:\n",
      "                    top_words = list(\n",
      "                        gammas[k].argsort()[-self.top_words:][::-1])\n",
      "                topic_words = [self.vocab[a] for a in top_words]\n",
      "                topic_w.append(topic_words)\n",
      "\n",
      "        info['topic-word-matrix'] = gammas\n",
      "        info['topic-document-matrix'] = theta.cpu().detach().numpy().T\n",
      "        info['topics'] = topic_w\n",
      "        return info\n",
      "\n",
      "    def inference(self):\n",
      "        assert isinstance(self.use_partitions, bool) and self.use_partitions\n",
      "        topic_d = []\n",
      "        self.model.eval()\n",
      "        indices = torch.arange(0, len(self.test_tokens))\n",
      "        indices = torch.split(indices, self.hyperparameters['batch_size'])\n",
      "\n",
      "        for idx, ind in enumerate(indices):\n",
      "            data_batch = data.get_batch(self.test_tokens, self.test_counts,\n",
      "                                        ind, len(self.vocab.keys()),\n",
      "                                        self.device)\n",
      "            sums = data_batch.sum(1).unsqueeze(1)\n",
      "            if self.hyperparameters['bow_norm']:\n",
      "                normalized_data_batch = data_batch / sums\n",
      "            else:\n",
      "                normalized_data_batch = data_batch\n",
      "            theta, _ = self.model.get_theta(normalized_data_batch)\n",
      "            topic_d.append(theta.cpu().detach().numpy())\n",
      "\n",
      "        info = self.get_info()\n",
      "        emp_array = np.empty((0, self.hyperparameters['num_topics']))\n",
      "\n",
      "        # batch concatenation\n",
      "        for i in range(len(topic_d)):\n",
      "            emp_array = np.concatenate([emp_array, topic_d[i]])\n",
      "        info['test-topic-document-matrix'] = emp_array.T\n",
      "\n",
      "        return info\n",
      "\n",
      "    def set_default_hyperparameters(self, hyperparameters):\n",
      "        for k in hyperparameters.keys():\n",
      "            if k in self.hyperparameters.keys():\n",
      "                self.hyperparameters[k] = hyperparameters.get(\n",
      "                    k, self.hyperparameters[k])\n",
      "\n",
      "    def partitioning(self, use_partitions=False):\n",
      "        self.use_partitions = use_partitions\n",
      "\n",
      "    @staticmethod\n",
      "    def preprocess(\n",
      "            vocab2id, train_corpus, test_corpus=None, validation_corpus=None):\n",
      "\n",
      "        def split_bow(bow_in, n_docs):\n",
      "            indices = [\n",
      "                [w for w in bow_in[doc, :].indices] for doc in range(n_docs)]\n",
      "            counts = [\n",
      "                [c for c in bow_in[doc, :].data] for doc in range(n_docs)]\n",
      "            return indices, counts\n",
      "\n",
      "        vec = CountVectorizer(\n",
      "            vocabulary=vocab2id, token_pattern=r'(?u)\\b\\w+\\b')\n",
      "\n",
      "        dataset = train_corpus.copy()\n",
      "        if test_corpus is not None:\n",
      "            dataset.extend(test_corpus)\n",
      "        if validation_corpus is not None:\n",
      "            dataset.extend(validation_corpus)\n",
      "\n",
      "        vec.fit(dataset)\n",
      "        idx2token = {v: k for (k, v) in vec.vocabulary_.items()}\n",
      "\n",
      "        x_train = vec.transform(train_corpus)\n",
      "        x_train_tokens, x_train_count = split_bow(x_train, x_train.shape[0])\n",
      "\n",
      "        if test_corpus is not None:\n",
      "            x_test = vec.transform(test_corpus)\n",
      "            x_test_tokens, x_test_count = split_bow(x_test, x_test.shape[0])\n",
      "\n",
      "            if validation_corpus is not None:\n",
      "                x_validation = vec.transform(validation_corpus)\n",
      "                x_val_tokens, x_val_count = split_bow(\n",
      "                    x_validation, x_validation.shape[0])\n",
      "\n",
      "                return (\n",
      "                    x_train_tokens, x_train_count, x_test_tokens,\n",
      "                    x_test_count, x_val_tokens, x_val_count)\n",
      "            else:\n",
      "                return (\n",
      "                    x_train_tokens, x_train_count, x_test_tokens, x_test_count)\n",
      "        else:\n",
      "            if validation_corpus is not None:\n",
      "                x_validation = vec.transform(validation_corpus)\n",
      "                x_val_tokens, x_val_count = split_bow(\n",
      "                    x_validation, x_validation.shape[0])\n",
      "                return x_train_tokens, x_train_count, x_val_tokens, x_val_count\n",
      "            else:\n",
      "                return x_train_tokens, x_train_count\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(octis.models.ETM.ETM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.load_custom_dataset_from_folder(\"content/corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = Coherence(texts=dataset.get_corpus(), measure='c_v')\n",
    "umass = Coherence(texts=dataset.get_corpus(), measure='u_mass')\n",
    "uci = Coherence(texts=dataset.get_corpus(), measure='c_uci')\n",
    "npmi = Coherence(texts=dataset.get_corpus())\n",
    "topic_diversity = TopicDiversity(topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "model = ETM(num_topics=7,use_partitions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.5, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=12147, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=7, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=12147, out_features=800, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=800, out_features=7, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=800, out_features=7, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 281.24 .. NELBO: 281.25\n",
      "Epoch: 1 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 246.0 .. NELBO: 246.01\n",
      "Epoch: 1 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 233.26 .. NELBO: 233.27\n",
      "Epoch: 1 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 235.72 .. NELBO: 235.73\n",
      "Epoch: 1 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 232.31 .. NELBO: 232.32\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 230.14 .. NELBO: 230.15\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 237.67 .. NELBO: 237.67\n",
      "Epoch: 2 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 221.42 .. NELBO: 221.42\n",
      "Epoch: 2 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 215.57 .. NELBO: 215.57\n",
      "Epoch: 2 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 221.4 .. NELBO: 221.4\n",
      "Epoch: 2 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 220.3 .. NELBO: 220.3\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 218.78 .. NELBO: 218.78\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 235.89 .. NELBO: 235.89\n",
      "Epoch: 3 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 220.43 .. NELBO: 220.43\n",
      "Epoch: 3 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 214.91 .. NELBO: 214.91\n",
      "Epoch: 3 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 220.48 .. NELBO: 220.48\n",
      "Epoch: 3 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 219.16 .. NELBO: 219.16\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 217.32 .. NELBO: 217.32\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 237.17 .. NELBO: 237.18\n",
      "Epoch: 4 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 220.53 .. NELBO: 220.54\n",
      "Epoch: 4 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 214.71 .. NELBO: 214.72\n",
      "Epoch: 4 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 220.04 .. NELBO: 220.05\n",
      "Epoch: 4 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 218.93 .. NELBO: 218.94\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 217.05 .. NELBO: 217.06\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 234.49 .. NELBO: 234.51\n",
      "Epoch: 5 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 218.82 .. NELBO: 218.84\n",
      "Epoch: 5 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 213.72 .. NELBO: 213.74\n",
      "Epoch: 5 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 219.2 .. NELBO: 219.22\n",
      "Epoch: 5 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 218.0 .. NELBO: 218.02\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 216.03 .. NELBO: 216.05\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 234.22 .. NELBO: 234.35\n",
      "Epoch: 6 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 218.28 .. NELBO: 218.36\n",
      "Epoch: 6 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 213.25 .. NELBO: 213.31\n",
      "Epoch: 6 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 218.73 .. NELBO: 218.78\n",
      "Epoch: 6 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 217.53 .. NELBO: 217.58\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 215.54 .. NELBO: 215.59\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 234.17 .. NELBO: 234.2\n",
      "Epoch: 7 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 218.13 .. NELBO: 218.18\n",
      "Epoch: 7 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 213.09 .. NELBO: 213.13\n",
      "Epoch: 7 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 218.57 .. NELBO: 218.61\n",
      "Epoch: 7 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 217.36 .. NELBO: 217.39\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 215.41 .. NELBO: 215.44\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 233.22 .. NELBO: 233.34\n",
      "Epoch: 8 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 217.72 .. NELBO: 217.81\n",
      "Epoch: 8 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 212.85 .. NELBO: 212.91\n",
      "Epoch: 8 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 218.27 .. NELBO: 218.32\n",
      "Epoch: 8 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 217.13 .. NELBO: 217.17\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 215.03 .. NELBO: 215.07\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 233.36 .. NELBO: 233.38\n",
      "Epoch: 9 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 217.5 .. NELBO: 217.52\n",
      "Epoch: 9 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 212.66 .. NELBO: 212.68\n",
      "Epoch: 9 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 218.11 .. NELBO: 218.12\n",
      "Epoch: 9 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 216.88 .. NELBO: 216.89\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 214.84 .. NELBO: 214.85\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 232.45 .. NELBO: 232.5\n",
      "Epoch: 10 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 217.03 .. NELBO: 217.07\n",
      "Epoch: 10 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 212.34 .. NELBO: 212.37\n",
      "Epoch: 10 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 217.73 .. NELBO: 217.75\n",
      "Epoch: 10 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 216.66 .. NELBO: 216.68\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 214.44 .. NELBO: 214.46\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 232.97 .. NELBO: 233.01\n",
      "Epoch: 11 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 217.14 .. NELBO: 217.17\n",
      "Epoch: 11 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 212.34 .. NELBO: 212.37\n",
      "Epoch: 11 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 217.79 .. NELBO: 217.81\n",
      "Epoch: 11 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 216.58 .. NELBO: 216.6\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 214.49 .. NELBO: 214.51\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 232.18 .. NELBO: 232.21\n",
      "Epoch: 12 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 216.74 .. NELBO: 216.77\n",
      "Epoch: 12 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 212.07 .. NELBO: 212.09\n",
      "Epoch: 12 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 217.5 .. NELBO: 217.52\n",
      "Epoch: 12 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 216.44 .. NELBO: 216.45\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 214.24 .. NELBO: 214.25\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 232.54 .. NELBO: 232.56\n",
      "Epoch: 13 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 216.94 .. NELBO: 216.96\n",
      "Epoch: 13 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 212.14 .. NELBO: 212.16\n",
      "Epoch: 13 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 217.63 .. NELBO: 217.64\n",
      "Epoch: 13 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 216.43 .. NELBO: 216.44\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 214.35 .. NELBO: 214.36\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 231.81 .. NELBO: 231.86\n",
      "Epoch: 14 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 216.6 .. NELBO: 216.65\n",
      "Epoch: 14 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 211.97 .. NELBO: 212.01\n",
      "Epoch: 14 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 217.39 .. NELBO: 217.42\n",
      "Epoch: 14 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 216.34 .. NELBO: 216.37\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 214.12 .. NELBO: 214.14\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 232.48 .. NELBO: 232.55\n",
      "Epoch: 15 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 216.93 .. NELBO: 217.03\n",
      "Epoch: 15 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 212.12 .. NELBO: 212.21\n",
      "Epoch: 15 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 217.63 .. NELBO: 217.7\n",
      "Epoch: 15 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 216.37 .. NELBO: 216.43\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 214.28 .. NELBO: 214.33\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 232.13 .. NELBO: 232.2\n",
      "Epoch: 16 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 216.8 .. NELBO: 216.89\n",
      "Epoch: 16 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 212.06 .. NELBO: 212.15\n",
      "Epoch: 16 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 217.48 .. NELBO: 217.56\n",
      "Epoch: 16 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 216.36 .. NELBO: 216.43\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 214.17 .. NELBO: 214.23\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 232.33 .. NELBO: 232.6\n",
      "Epoch: 17 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 216.93 .. NELBO: 217.14\n",
      "Epoch: 17 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 212.16 .. NELBO: 212.33\n",
      "Epoch: 17 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 217.71 .. NELBO: 217.85\n",
      "Epoch: 17 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 216.5 .. NELBO: 216.62\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 214.36 .. NELBO: 214.47\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 232.38 .. NELBO: 232.85\n",
      "Epoch: 18 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.29 .. Rec_loss: 217.01 .. NELBO: 217.3\n",
      "Epoch: 18 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 212.21 .. NELBO: 212.42\n",
      "Epoch: 18 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 217.77 .. NELBO: 217.95\n",
      "Epoch: 18 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 216.6 .. NELBO: 216.76\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 214.43 .. NELBO: 214.57\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 232.66 .. NELBO: 232.83\n",
      "Epoch: 19 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 217.28 .. NELBO: 217.4\n",
      "Epoch: 19 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 212.57 .. NELBO: 212.67\n",
      "Epoch: 19 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 218.06 .. NELBO: 218.15\n",
      "Epoch: 19 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 216.81 .. NELBO: 216.89\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 214.73 .. NELBO: 214.81\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 232.04 .. NELBO: 232.11\n",
      "Epoch: 20 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 216.84 .. NELBO: 216.9\n",
      "Epoch: 20 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 212.29 .. NELBO: 212.35\n",
      "Epoch: 20 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 217.8 .. NELBO: 217.85\n",
      "Epoch: 20 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 216.65 .. NELBO: 216.7\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 214.43 .. NELBO: 214.47\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 232.28 .. NELBO: 232.34\n",
      "Epoch: 21 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 216.85 .. NELBO: 216.93\n",
      "Epoch: 21 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 212.4 .. NELBO: 212.48\n",
      "Epoch: 21 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 217.74 .. NELBO: 217.81\n",
      "Epoch: 21 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 216.49 .. NELBO: 216.55\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 214.4 .. NELBO: 214.46\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 232.12 .. NELBO: 232.29\n",
      "Epoch: 22 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 216.83 .. NELBO: 216.97\n",
      "Epoch: 22 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 212.21 .. NELBO: 212.33\n",
      "Epoch: 22 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 217.52 .. NELBO: 217.62\n",
      "Epoch: 22 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 216.38 .. NELBO: 216.47\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 214.15 .. NELBO: 214.23\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 232.75 .. NELBO: 232.89\n",
      "Epoch: 23 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 217.09 .. NELBO: 217.25\n",
      "Epoch: 23 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 212.26 .. NELBO: 212.4\n",
      "Epoch: 23 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 217.6 .. NELBO: 217.73\n",
      "Epoch: 23 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 216.35 .. NELBO: 216.47\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 214.23 .. NELBO: 214.35\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.86 .. Rec_loss: 232.23 .. NELBO: 233.09\n",
      "Epoch: 24 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 216.91 .. NELBO: 217.43\n",
      "Epoch: 24 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.38 .. Rec_loss: 212.16 .. NELBO: 212.54\n",
      "Epoch: 24 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.31 .. Rec_loss: 217.59 .. NELBO: 217.9\n",
      "Epoch: 24 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 216.5 .. NELBO: 216.77\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 214.19 .. NELBO: 214.43\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 232.77 .. NELBO: 232.89\n",
      "Epoch: 25 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 217.24 .. NELBO: 217.41\n",
      "Epoch: 25 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 212.39 .. NELBO: 212.56\n",
      "Epoch: 25 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 217.86 .. NELBO: 218.03\n",
      "Epoch: 25 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 216.54 .. NELBO: 216.7\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 214.39 .. NELBO: 214.54\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.78 .. Rec_loss: 232.73 .. NELBO: 233.51\n",
      "Epoch: 26 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 217.06 .. NELBO: 217.54\n",
      "Epoch: 26 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.36 .. Rec_loss: 212.25 .. NELBO: 212.61\n",
      "Epoch: 26 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.29 .. Rec_loss: 217.71 .. NELBO: 218.0\n",
      "Epoch: 26 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.25 .. Rec_loss: 216.55 .. NELBO: 216.8\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 214.33 .. NELBO: 214.56\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 232.95 .. NELBO: 233.04\n",
      "Epoch: 27 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 217.33 .. NELBO: 217.45\n",
      "Epoch: 27 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 212.55 .. NELBO: 212.68\n",
      "Epoch: 27 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 218.0 .. NELBO: 218.14\n",
      "Epoch: 27 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 216.67 .. NELBO: 216.8\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 214.56 .. NELBO: 214.68\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 233.06 .. NELBO: 233.11\n",
      "Epoch: 28 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 217.35 .. NELBO: 217.43\n",
      "Epoch: 28 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 212.58 .. NELBO: 212.67\n",
      "Epoch: 28 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 217.91 .. NELBO: 218.01\n",
      "Epoch: 28 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 216.6 .. NELBO: 216.71\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 214.52 .. NELBO: 214.63\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.83 .. Rec_loss: 232.56 .. NELBO: 233.39\n",
      "Epoch: 29 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 217.17 .. NELBO: 217.68\n",
      "Epoch: 29 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.38 .. Rec_loss: 212.43 .. NELBO: 212.81\n",
      "Epoch: 29 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.31 .. Rec_loss: 217.77 .. NELBO: 218.08\n",
      "Epoch: 29 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 216.7 .. NELBO: 216.97\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 214.55 .. NELBO: 214.79\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 232.99 .. NELBO: 233.1\n",
      "Epoch: 30 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 217.36 .. NELBO: 217.51\n",
      "Epoch: 30 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 212.77 .. NELBO: 212.92\n",
      "Epoch: 30 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 218.11 .. NELBO: 218.25\n",
      "Epoch: 30 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 216.82 .. NELBO: 216.95\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 214.72 .. NELBO: 214.86\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 232.65 .. NELBO: 233.15\n",
      "Epoch: 31 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.34 .. Rec_loss: 217.18 .. NELBO: 217.52\n",
      "Epoch: 31 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 212.51 .. NELBO: 212.78\n",
      "Epoch: 31 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 217.82 .. NELBO: 218.06\n",
      "Epoch: 31 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 216.66 .. NELBO: 216.87\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 214.42 .. NELBO: 214.61\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.41 .. Rec_loss: 232.42 .. NELBO: 232.83\n",
      "Epoch: 32 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.31 .. Rec_loss: 217.1 .. NELBO: 217.41\n",
      "Epoch: 32 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.25 .. Rec_loss: 212.41 .. NELBO: 212.66\n",
      "Epoch: 32 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 217.81 .. NELBO: 218.02\n",
      "Epoch: 32 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 216.6 .. NELBO: 216.79\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 214.38 .. NELBO: 214.57\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 233.01 .. NELBO: 233.11\n",
      "Epoch: 33 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 217.51 .. NELBO: 217.66\n",
      "Epoch: 33 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 212.8 .. NELBO: 212.96\n",
      "Epoch: 33 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 218.13 .. NELBO: 218.29\n",
      "Epoch: 33 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 216.77 .. NELBO: 216.91\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 214.6 .. NELBO: 214.73\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.38 .. Rec_loss: 233.04 .. NELBO: 233.42\n",
      "Epoch: 34 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 217.38 .. NELBO: 217.65\n",
      "Epoch: 34 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 212.56 .. NELBO: 212.79\n",
      "Epoch: 34 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 217.85 .. NELBO: 218.08\n",
      "Epoch: 34 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 216.67 .. NELBO: 216.87\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 214.5 .. NELBO: 214.69\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 232.97 .. NELBO: 233.24\n",
      "Epoch: 35 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 217.4 .. NELBO: 217.63\n",
      "Epoch: 35 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.22 .. Rec_loss: 212.6 .. NELBO: 212.82\n",
      "Epoch: 35 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.22 .. Rec_loss: 217.91 .. NELBO: 218.13\n",
      "Epoch: 35 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 216.66 .. NELBO: 216.85\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 214.62 .. NELBO: 214.79\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.33 .. Rec_loss: 233.46 .. NELBO: 233.79\n",
      "Epoch: 36 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 217.66 .. NELBO: 217.87\n",
      "Epoch: 36 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 212.89 .. NELBO: 213.06\n",
      "Epoch: 36 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 218.09 .. NELBO: 218.24\n",
      "Epoch: 36 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 216.88 .. NELBO: 217.01\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 214.77 .. NELBO: 214.89\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 232.66 .. NELBO: 232.89\n",
      "Epoch: 37 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 217.19 .. NELBO: 217.38\n",
      "Epoch: 37 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 212.47 .. NELBO: 212.66\n",
      "Epoch: 37 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 217.74 .. NELBO: 217.94\n",
      "Epoch: 37 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 216.59 .. NELBO: 216.77\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 214.52 .. NELBO: 214.7\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 232.8 .. NELBO: 233.39\n",
      "Epoch: 38 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.4 .. Rec_loss: 217.31 .. NELBO: 217.71\n",
      "Epoch: 38 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.31 .. Rec_loss: 212.63 .. NELBO: 212.94\n",
      "Epoch: 38 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.28 .. Rec_loss: 217.86 .. NELBO: 218.14\n",
      "Epoch: 38 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 216.69 .. NELBO: 216.93\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 214.5 .. NELBO: 214.73\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.35 .. Rec_loss: 232.74 .. NELBO: 233.09\n",
      "Epoch: 39 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.25 .. Rec_loss: 217.27 .. NELBO: 217.52\n",
      "Epoch: 39 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 212.51 .. NELBO: 212.74\n",
      "Epoch: 39 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 217.85 .. NELBO: 218.06\n",
      "Epoch: 39 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 216.61 .. NELBO: 216.81\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 214.42 .. NELBO: 214.61\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.61 .. Rec_loss: 232.96 .. NELBO: 233.57\n",
      "Epoch: 40 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.42 .. Rec_loss: 217.45 .. NELBO: 217.87\n",
      "Epoch: 40 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.33 .. Rec_loss: 212.61 .. NELBO: 212.94\n",
      "Epoch: 40 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.28 .. Rec_loss: 217.96 .. NELBO: 218.24\n",
      "Epoch: 40 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 216.77 .. NELBO: 217.01\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 214.45 .. NELBO: 214.71\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 232.92 .. NELBO: 233.51\n",
      "Epoch: 41 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.39 .. Rec_loss: 217.29 .. NELBO: 217.68\n",
      "Epoch: 41 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.34 .. Rec_loss: 212.42 .. NELBO: 212.76\n",
      "Epoch: 41 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.3 .. Rec_loss: 217.89 .. NELBO: 218.19\n",
      "Epoch: 41 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.29 .. Rec_loss: 216.71 .. NELBO: 217.0\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.28 .. Rec_loss: 214.65 .. NELBO: 214.93\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.81 .. Rec_loss: 232.99 .. NELBO: 233.8\n",
      "Epoch: 42 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.55 .. Rec_loss: 217.33 .. NELBO: 217.88\n",
      "Epoch: 42 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.43 .. Rec_loss: 212.38 .. NELBO: 212.81\n",
      "Epoch: 42 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.38 .. Rec_loss: 217.86 .. NELBO: 218.24\n",
      "Epoch: 42 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.34 .. Rec_loss: 216.69 .. NELBO: 217.03\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.31 .. Rec_loss: 214.59 .. NELBO: 214.9\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.37 .. Rec_loss: 233.19 .. NELBO: 233.56\n",
      "Epoch: 43 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 217.56 .. NELBO: 217.82\n",
      "Epoch: 43 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 212.72 .. NELBO: 212.95\n",
      "Epoch: 43 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.22 .. Rec_loss: 218.29 .. NELBO: 218.51\n",
      "Epoch: 43 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 217.04 .. NELBO: 217.24\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 214.99 .. NELBO: 215.17\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 233.91 .. NELBO: 234.04\n",
      "Epoch: 44 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 218.17 .. NELBO: 218.3\n",
      "Epoch: 44 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 213.23 .. NELBO: 213.38\n",
      "Epoch: 44 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 218.64 .. NELBO: 218.81\n",
      "Epoch: 44 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 217.32 .. NELBO: 217.48\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 215.29 .. NELBO: 215.44\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 233.16 .. NELBO: 233.3\n",
      "Epoch: 45 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 217.76 .. NELBO: 217.91\n",
      "Epoch: 45 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 213.04 .. NELBO: 213.22\n",
      "Epoch: 45 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 218.47 .. NELBO: 218.63\n",
      "Epoch: 45 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 217.28 .. NELBO: 217.42\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 215.1 .. NELBO: 215.25\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 233.06 .. NELBO: 233.32\n",
      "Epoch: 46 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 217.67 .. NELBO: 217.88\n",
      "Epoch: 46 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 213.34 .. NELBO: 213.54\n",
      "Epoch: 46 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 218.57 .. NELBO: 218.76\n",
      "Epoch: 46 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 217.36 .. NELBO: 217.53\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 215.29 .. NELBO: 215.45\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 233.81 .. NELBO: 233.89\n",
      "Epoch: 47 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 217.87 .. NELBO: 217.98\n",
      "Epoch: 47 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 213.54 .. NELBO: 213.67\n",
      "Epoch: 47 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 218.59 .. NELBO: 218.71\n",
      "Epoch: 47 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 217.3 .. NELBO: 217.42\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 215.24 .. NELBO: 215.35\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 233.98 .. NELBO: 234.3\n",
      "Epoch: 48 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.25 .. Rec_loss: 218.08 .. NELBO: 218.33\n",
      "Epoch: 48 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 213.4 .. NELBO: 213.64\n",
      "Epoch: 48 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.22 .. Rec_loss: 218.49 .. NELBO: 218.71\n",
      "Epoch: 48 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 217.19 .. NELBO: 217.39\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 215.13 .. NELBO: 215.32\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 233.37 .. NELBO: 233.61\n",
      "Epoch: 49 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.22 .. Rec_loss: 217.58 .. NELBO: 217.8\n",
      "Epoch: 49 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 212.92 .. NELBO: 213.13\n",
      "Epoch: 49 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 218.15 .. NELBO: 218.36\n",
      "Epoch: 49 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 216.89 .. NELBO: 217.08\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 214.79 .. NELBO: 214.97\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.44 .. Rec_loss: 233.11 .. NELBO: 233.55\n",
      "Epoch: 50 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.31 .. Rec_loss: 217.44 .. NELBO: 217.75\n",
      "Epoch: 50 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.28 .. Rec_loss: 212.76 .. NELBO: 213.04\n",
      "Epoch: 50 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 218.0 .. NELBO: 218.26\n",
      "Epoch: 50 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 216.77 .. NELBO: 217.0\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 214.67 .. NELBO: 214.88\n",
      "****************************************************************************************************\n",
      "Epoch: 51 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.39 .. Rec_loss: 233.35 .. NELBO: 233.74\n",
      "Epoch: 51 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.31 .. Rec_loss: 217.52 .. NELBO: 217.83\n",
      "Epoch: 51 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.28 .. Rec_loss: 212.7 .. NELBO: 212.98\n",
      "Epoch: 51 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.25 .. Rec_loss: 218.0 .. NELBO: 218.25\n",
      "Epoch: 51 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.22 .. Rec_loss: 216.73 .. NELBO: 216.95\n",
      "****************************************************************************************************\n",
      "Epoch----->51 .. LR: 0.005 .. KL_theta: 0.22 .. Rec_loss: 214.84 .. NELBO: 215.06\n",
      "****************************************************************************************************\n",
      "Epoch: 52 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 233.54 .. NELBO: 233.66\n",
      "Epoch: 52 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 217.62 .. NELBO: 217.78\n",
      "Epoch: 52 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 212.79 .. NELBO: 212.96\n",
      "Epoch: 52 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 218.08 .. NELBO: 218.26\n",
      "Epoch: 52 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 216.78 .. NELBO: 216.94\n",
      "****************************************************************************************************\n",
      "Epoch----->52 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 214.85 .. NELBO: 215.0\n",
      "****************************************************************************************************\n",
      "Epoch: 53 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 233.14 .. NELBO: 233.46\n",
      "Epoch: 53 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.22 .. Rec_loss: 217.65 .. NELBO: 217.87\n",
      "Epoch: 53 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 212.76 .. NELBO: 212.96\n",
      "Epoch: 53 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 218.01 .. NELBO: 218.22\n",
      "Epoch: 53 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 216.82 .. NELBO: 217.0\n",
      "****************************************************************************************************\n",
      "Epoch----->53 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 214.76 .. NELBO: 214.93\n",
      "****************************************************************************************************\n",
      "Epoch: 54 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 233.49 .. NELBO: 233.59\n",
      "Epoch: 54 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 217.68 .. NELBO: 217.82\n",
      "Epoch: 54 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 213.0 .. NELBO: 213.16\n",
      "Epoch: 54 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 218.1 .. NELBO: 218.28\n",
      "Epoch: 54 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 216.82 .. NELBO: 216.98\n",
      "****************************************************************************************************\n",
      "Epoch----->54 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 214.8 .. NELBO: 214.97\n",
      "****************************************************************************************************\n",
      "Epoch: 55 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.68 .. Rec_loss: 233.21 .. NELBO: 233.89\n",
      "Epoch: 55 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 217.56 .. NELBO: 218.03\n",
      "Epoch: 55 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.37 .. Rec_loss: 212.8 .. NELBO: 213.17\n",
      "Epoch: 55 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.37 .. Rec_loss: 217.87 .. NELBO: 218.24\n",
      "Epoch: 55 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.33 .. Rec_loss: 216.74 .. NELBO: 217.07\n",
      "****************************************************************************************************\n",
      "Epoch----->55 .. LR: 0.005 .. KL_theta: 0.33 .. Rec_loss: 214.54 .. NELBO: 214.87\n",
      "****************************************************************************************************\n",
      "Epoch: 56 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 232.56 .. NELBO: 233.16\n",
      "Epoch: 56 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 217.13 .. NELBO: 217.61\n",
      "Epoch: 56 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.41 .. Rec_loss: 212.41 .. NELBO: 212.82\n",
      "Epoch: 56 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.38 .. Rec_loss: 217.68 .. NELBO: 218.06\n",
      "Epoch: 56 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.33 .. Rec_loss: 216.62 .. NELBO: 216.95\n",
      "****************************************************************************************************\n",
      "Epoch----->56 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 214.51 .. NELBO: 214.83\n",
      "****************************************************************************************************\n",
      "Epoch: 57 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 233.64 .. NELBO: 233.79\n",
      "Epoch: 57 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 217.57 .. NELBO: 217.77\n",
      "Epoch: 57 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.22 .. Rec_loss: 212.7 .. NELBO: 212.92\n",
      "Epoch: 57 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 218.01 .. NELBO: 218.24\n",
      "Epoch: 57 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 216.73 .. NELBO: 216.94\n",
      "****************************************************************************************************\n",
      "Epoch----->57 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 214.67 .. NELBO: 214.87\n",
      "****************************************************************************************************\n",
      "Epoch: 58 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 233.25 .. NELBO: 233.42\n",
      "Epoch: 58 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 217.45 .. NELBO: 217.66\n",
      "Epoch: 58 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 212.7 .. NELBO: 212.9\n",
      "Epoch: 58 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 217.98 .. NELBO: 218.21\n",
      "Epoch: 58 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 216.74 .. NELBO: 216.95\n",
      "****************************************************************************************************\n",
      "Epoch----->58 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 214.64 .. NELBO: 214.84\n",
      "****************************************************************************************************\n",
      "Epoch: 59 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.31 .. Rec_loss: 233.33 .. NELBO: 233.64\n",
      "Epoch: 59 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.22 .. Rec_loss: 217.67 .. NELBO: 217.89\n",
      "Epoch: 59 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 212.7 .. NELBO: 212.91\n",
      "Epoch: 59 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 217.98 .. NELBO: 218.21\n",
      "Epoch: 59 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 216.69 .. NELBO: 216.89\n",
      "****************************************************************************************************\n",
      "Epoch----->59 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 214.55 .. NELBO: 214.75\n",
      "****************************************************************************************************\n",
      "Epoch: 60 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.39 .. Rec_loss: 232.7 .. NELBO: 233.09\n",
      "Epoch: 60 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 217.27 .. NELBO: 217.59\n",
      "Epoch: 60 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 212.42 .. NELBO: 212.74\n",
      "Epoch: 60 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.36 .. Rec_loss: 217.7 .. NELBO: 218.06\n",
      "Epoch: 60 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.31 .. Rec_loss: 216.57 .. NELBO: 216.88\n",
      "****************************************************************************************************\n",
      "Epoch----->60 .. LR: 0.005 .. KL_theta: 0.29 .. Rec_loss: 214.47 .. NELBO: 214.76\n",
      "****************************************************************************************************\n",
      "Epoch: 61 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 232.96 .. NELBO: 233.47\n",
      "Epoch: 61 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.4 .. Rec_loss: 217.37 .. NELBO: 217.77\n",
      "Epoch: 61 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.36 .. Rec_loss: 212.45 .. NELBO: 212.81\n",
      "Epoch: 61 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.41 .. Rec_loss: 217.73 .. NELBO: 218.14\n",
      "Epoch: 61 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.35 .. Rec_loss: 216.64 .. NELBO: 216.99\n",
      "****************************************************************************************************\n",
      "Epoch----->61 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 214.56 .. NELBO: 214.88\n",
      "****************************************************************************************************\n",
      "Epoch: 62 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 233.37 .. NELBO: 233.47\n",
      "Epoch: 62 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 217.58 .. NELBO: 217.7\n",
      "Epoch: 62 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 212.83 .. NELBO: 212.96\n",
      "Epoch: 62 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 218.08 .. NELBO: 218.22\n",
      "Epoch: 62 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 216.83 .. NELBO: 216.97\n",
      "****************************************************************************************************\n",
      "Epoch----->62 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 214.76 .. NELBO: 214.9\n",
      "****************************************************************************************************\n",
      "Epoch: 63 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.75 .. Rec_loss: 233.53 .. NELBO: 234.28\n",
      "Epoch: 63 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 217.62 .. NELBO: 218.1\n",
      "Epoch: 63 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.37 .. Rec_loss: 212.73 .. NELBO: 213.1\n",
      "Epoch: 63 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.41 .. Rec_loss: 217.87 .. NELBO: 218.28\n",
      "Epoch: 63 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.35 .. Rec_loss: 216.73 .. NELBO: 217.08\n",
      "****************************************************************************************************\n",
      "Epoch----->63 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 214.62 .. NELBO: 214.94\n",
      "****************************************************************************************************\n",
      "Epoch: 64 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 234.57 .. NELBO: 234.69\n",
      "Epoch: 64 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 218.12 .. NELBO: 218.3\n",
      "Epoch: 64 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 213.01 .. NELBO: 213.22\n",
      "Epoch: 64 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 218.08 .. NELBO: 218.35\n",
      "Epoch: 64 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 216.81 .. NELBO: 217.05\n",
      "****************************************************************************************************\n",
      "Epoch----->64 .. LR: 0.005 .. KL_theta: 0.22 .. Rec_loss: 214.83 .. NELBO: 215.05\n",
      "****************************************************************************************************\n",
      "Epoch: 65 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 234.22 .. NELBO: 234.32\n",
      "Epoch: 65 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 218.03 .. NELBO: 218.16\n",
      "Epoch: 65 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 213.07 .. NELBO: 213.22\n",
      "Epoch: 65 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 218.31 .. NELBO: 218.47\n",
      "Epoch: 65 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 216.97 .. NELBO: 217.14\n",
      "****************************************************************************************************\n",
      "Epoch----->65 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 214.95 .. NELBO: 215.12\n",
      "****************************************************************************************************\n",
      "Epoch: 66 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 233.02 .. NELBO: 233.28\n",
      "Epoch: 66 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.22 .. Rec_loss: 217.63 .. NELBO: 217.85\n",
      "Epoch: 66 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 212.76 .. NELBO: 212.96\n",
      "Epoch: 66 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 218.06 .. NELBO: 218.26\n",
      "Epoch: 66 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 216.89 .. NELBO: 217.1\n",
      "****************************************************************************************************\n",
      "Epoch----->66 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 214.72 .. NELBO: 214.92\n",
      "****************************************************************************************************\n",
      "Epoch: 67 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 233.15 .. NELBO: 233.65\n",
      "Epoch: 67 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.4 .. Rec_loss: 217.62 .. NELBO: 218.02\n",
      "Epoch: 67 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 212.72 .. NELBO: 213.04\n",
      "Epoch: 67 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.34 .. Rec_loss: 217.9 .. NELBO: 218.24\n",
      "Epoch: 67 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.3 .. Rec_loss: 216.69 .. NELBO: 216.99\n",
      "****************************************************************************************************\n",
      "Epoch----->67 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 214.72 .. NELBO: 214.99\n",
      "****************************************************************************************************\n",
      "Epoch: 68 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 234.22 .. NELBO: 234.33\n",
      "Epoch: 68 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 218.19 .. NELBO: 218.32\n",
      "Epoch: 68 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 213.08 .. NELBO: 213.25\n",
      "Epoch: 68 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 218.18 .. NELBO: 218.37\n",
      "Epoch: 68 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 216.89 .. NELBO: 217.08\n",
      "****************************************************************************************************\n",
      "Epoch----->68 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 215.03 .. NELBO: 215.21\n",
      "****************************************************************************************************\n",
      "Epoch: 69 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 233.47 .. NELBO: 233.96\n",
      "Epoch: 69 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 217.84 .. NELBO: 218.16\n",
      "Epoch: 69 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 213.01 .. NELBO: 213.28\n",
      "Epoch: 69 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.29 .. Rec_loss: 218.01 .. NELBO: 218.3\n",
      "Epoch: 69 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.29 .. Rec_loss: 216.87 .. NELBO: 217.16\n",
      "****************************************************************************************************\n",
      "Epoch----->69 .. LR: 0.005 .. KL_theta: 0.28 .. Rec_loss: 214.73 .. NELBO: 215.01\n",
      "****************************************************************************************************\n",
      "Epoch: 70 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.76 .. Rec_loss: 233.87 .. NELBO: 234.63\n",
      "Epoch: 70 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 218.03 .. NELBO: 218.5\n",
      "Epoch: 70 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.38 .. Rec_loss: 213.27 .. NELBO: 213.65\n",
      "Epoch: 70 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.37 .. Rec_loss: 218.12 .. NELBO: 218.49\n",
      "Epoch: 70 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.31 .. Rec_loss: 217.03 .. NELBO: 217.34\n",
      "****************************************************************************************************\n",
      "Epoch----->70 .. LR: 0.005 .. KL_theta: 0.29 .. Rec_loss: 215.0 .. NELBO: 215.29\n",
      "****************************************************************************************************\n",
      "Epoch: 71 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.28 .. Rec_loss: 235.7 .. NELBO: 235.98\n",
      "Epoch: 71 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.3 .. Rec_loss: 218.78 .. NELBO: 219.08\n",
      "Epoch: 71 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 213.69 .. NELBO: 213.95\n",
      "Epoch: 71 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 218.61 .. NELBO: 218.87\n",
      "Epoch: 71 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 217.34 .. NELBO: 217.58\n",
      "****************************************************************************************************\n",
      "Epoch----->71 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 215.45 .. NELBO: 215.68\n",
      "****************************************************************************************************\n",
      "Epoch: 72 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.42 .. Rec_loss: 234.05 .. NELBO: 234.47\n",
      "Epoch: 72 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 217.99 .. NELBO: 218.31\n",
      "Epoch: 72 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.29 .. Rec_loss: 212.94 .. NELBO: 213.23\n",
      "Epoch: 72 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.34 .. Rec_loss: 218.03 .. NELBO: 218.37\n",
      "Epoch: 72 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.29 .. Rec_loss: 216.84 .. NELBO: 217.13\n",
      "****************************************************************************************************\n",
      "Epoch----->72 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 214.86 .. NELBO: 215.13\n",
      "****************************************************************************************************\n",
      "Epoch: 73 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 233.3 .. NELBO: 233.42\n",
      "Epoch: 73 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 217.84 .. NELBO: 218.01\n",
      "Epoch: 73 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 212.82 .. NELBO: 213.0\n",
      "Epoch: 73 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 218.11 .. NELBO: 218.32\n",
      "Epoch: 73 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 216.89 .. NELBO: 217.1\n",
      "****************************************************************************************************\n",
      "Epoch----->73 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 214.83 .. NELBO: 215.04\n",
      "****************************************************************************************************\n",
      "Epoch: 74 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 233.63 .. NELBO: 234.12\n",
      "Epoch: 74 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.36 .. Rec_loss: 217.92 .. NELBO: 218.28\n",
      "Epoch: 74 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.31 .. Rec_loss: 212.92 .. NELBO: 213.23\n",
      "Epoch: 74 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.33 .. Rec_loss: 218.02 .. NELBO: 218.35\n",
      "Epoch: 74 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.31 .. Rec_loss: 216.76 .. NELBO: 217.07\n",
      "****************************************************************************************************\n",
      "Epoch----->74 .. LR: 0.005 .. KL_theta: 0.31 .. Rec_loss: 214.81 .. NELBO: 215.12\n",
      "****************************************************************************************************\n",
      "Epoch: 75 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 233.7 .. NELBO: 233.97\n",
      "Epoch: 75 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.22 .. Rec_loss: 217.81 .. NELBO: 218.03\n",
      "Epoch: 75 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.25 .. Rec_loss: 212.8 .. NELBO: 213.05\n",
      "Epoch: 75 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.3 .. Rec_loss: 217.86 .. NELBO: 218.16\n",
      "Epoch: 75 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 216.78 .. NELBO: 217.04\n",
      "****************************************************************************************************\n",
      "Epoch----->75 .. LR: 0.005 .. KL_theta: 0.25 .. Rec_loss: 214.74 .. NELBO: 214.99\n",
      "****************************************************************************************************\n",
      "Epoch: 76 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 233.24 .. NELBO: 233.76\n",
      "Epoch: 76 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.41 .. Rec_loss: 217.59 .. NELBO: 218.0\n",
      "Epoch: 76 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.36 .. Rec_loss: 212.6 .. NELBO: 212.96\n",
      "Epoch: 76 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.4 .. Rec_loss: 217.71 .. NELBO: 218.11\n",
      "Epoch: 76 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.37 .. Rec_loss: 216.62 .. NELBO: 216.99\n",
      "****************************************************************************************************\n",
      "Epoch----->76 .. LR: 0.005 .. KL_theta: 0.39 .. Rec_loss: 214.64 .. NELBO: 215.03\n",
      "****************************************************************************************************\n",
      "Epoch: 77 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.68 .. Rec_loss: 233.31 .. NELBO: 233.99\n",
      "Epoch: 77 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.43 .. Rec_loss: 217.82 .. NELBO: 218.25\n",
      "Epoch: 77 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.37 .. Rec_loss: 212.86 .. NELBO: 213.23\n",
      "Epoch: 77 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.4 .. Rec_loss: 217.98 .. NELBO: 218.38\n",
      "Epoch: 77 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.35 .. Rec_loss: 216.86 .. NELBO: 217.21\n",
      "****************************************************************************************************\n",
      "Epoch----->77 .. LR: 0.005 .. KL_theta: 0.34 .. Rec_loss: 214.95 .. NELBO: 215.29\n",
      "****************************************************************************************************\n",
      "Epoch: 78 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 233.91 .. NELBO: 234.23\n",
      "Epoch: 78 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 218.01 .. NELBO: 218.28\n",
      "Epoch: 78 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.29 .. Rec_loss: 212.96 .. NELBO: 213.25\n",
      "Epoch: 78 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.3 .. Rec_loss: 218.19 .. NELBO: 218.49\n",
      "Epoch: 78 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 217.01 .. NELBO: 217.33\n",
      "****************************************************************************************************\n",
      "Epoch----->78 .. LR: 0.005 .. KL_theta: 0.34 .. Rec_loss: 215.06 .. NELBO: 215.4\n",
      "****************************************************************************************************\n",
      "Epoch: 79 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 232.78 .. NELBO: 233.37\n",
      "Epoch: 79 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.43 .. Rec_loss: 217.62 .. NELBO: 218.05\n",
      "Epoch: 79 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.36 .. Rec_loss: 212.83 .. NELBO: 213.19\n",
      "Epoch: 79 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.35 .. Rec_loss: 218.2 .. NELBO: 218.55\n",
      "Epoch: 79 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 217.06 .. NELBO: 217.38\n",
      "****************************************************************************************************\n",
      "Epoch----->79 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 214.99 .. NELBO: 215.31\n",
      "****************************************************************************************************\n",
      "Epoch: 80 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 234.07 .. NELBO: 234.39\n",
      "Epoch: 80 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.3 .. Rec_loss: 218.1 .. NELBO: 218.4\n",
      "Epoch: 80 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.33 .. Rec_loss: 212.92 .. NELBO: 213.25\n",
      "Epoch: 80 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.36 .. Rec_loss: 218.25 .. NELBO: 218.61\n",
      "Epoch: 80 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.33 .. Rec_loss: 216.96 .. NELBO: 217.29\n",
      "****************************************************************************************************\n",
      "Epoch----->80 .. LR: 0.005 .. KL_theta: 0.33 .. Rec_loss: 215.25 .. NELBO: 215.58\n",
      "****************************************************************************************************\n",
      "Epoch: 81 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.25 .. Rec_loss: 232.75 .. NELBO: 233.0\n",
      "Epoch: 81 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.29 .. Rec_loss: 217.77 .. NELBO: 218.06\n",
      "Epoch: 81 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 213.22 .. NELBO: 213.46\n",
      "Epoch: 81 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.22 .. Rec_loss: 218.41 .. NELBO: 218.63\n",
      "Epoch: 81 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 217.29 .. NELBO: 217.48\n",
      "****************************************************************************************************\n",
      "Epoch----->81 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 215.21 .. NELBO: 215.38\n",
      "****************************************************************************************************\n",
      "Epoch: 82 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 235.03 .. NELBO: 235.18\n",
      "Epoch: 82 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 218.57 .. NELBO: 218.72\n",
      "Epoch: 82 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 213.67 .. NELBO: 213.8\n",
      "Epoch: 82 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 218.67 .. NELBO: 218.84\n",
      "Epoch: 82 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 217.37 .. NELBO: 217.53\n",
      "****************************************************************************************************\n",
      "Epoch----->82 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 215.51 .. NELBO: 215.68\n",
      "****************************************************************************************************\n",
      "Epoch: 83 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.25 .. Rec_loss: 232.81 .. NELBO: 233.06\n",
      "Epoch: 83 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 217.56 .. NELBO: 217.77\n",
      "Epoch: 83 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 213.03 .. NELBO: 213.22\n",
      "Epoch: 83 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 218.23 .. NELBO: 218.41\n",
      "Epoch: 83 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 217.12 .. NELBO: 217.28\n",
      "****************************************************************************************************\n",
      "Epoch----->83 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 215.06 .. NELBO: 215.23\n",
      "****************************************************************************************************\n",
      "Epoch: 84 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.33 .. Rec_loss: 233.49 .. NELBO: 233.82\n",
      "Epoch: 84 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.29 .. Rec_loss: 217.84 .. NELBO: 218.13\n",
      "Epoch: 84 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 213.08 .. NELBO: 213.32\n",
      "Epoch: 84 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 218.37 .. NELBO: 218.58\n",
      "Epoch: 84 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 217.18 .. NELBO: 217.37\n",
      "****************************************************************************************************\n",
      "Epoch----->84 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 215.16 .. NELBO: 215.34\n",
      "****************************************************************************************************\n",
      "Epoch: 85 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 233.96 .. NELBO: 234.07\n",
      "Epoch: 85 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 218.08 .. NELBO: 218.22\n",
      "Epoch: 85 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 213.21 .. NELBO: 213.35\n",
      "Epoch: 85 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 218.45 .. NELBO: 218.58\n",
      "Epoch: 85 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 217.17 .. NELBO: 217.31\n",
      "****************************************************************************************************\n",
      "Epoch----->85 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 215.25 .. NELBO: 215.4\n",
      "****************************************************************************************************\n",
      "Epoch: 86 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.33 .. Rec_loss: 233.47 .. NELBO: 233.8\n",
      "Epoch: 86 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 217.84 .. NELBO: 218.05\n",
      "Epoch: 86 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 212.98 .. NELBO: 213.17\n",
      "Epoch: 86 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 218.23 .. NELBO: 218.4\n",
      "Epoch: 86 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 217.11 .. NELBO: 217.3\n",
      "****************************************************************************************************\n",
      "Epoch----->86 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 215.04 .. NELBO: 215.28\n",
      "****************************************************************************************************\n",
      "Epoch: 87 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.39 .. Rec_loss: 232.5 .. NELBO: 232.89\n",
      "Epoch: 87 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 217.15 .. NELBO: 217.38\n",
      "Epoch: 87 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 212.3 .. NELBO: 212.54\n",
      "Epoch: 87 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 217.87 .. NELBO: 218.14\n",
      "Epoch: 87 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.25 .. Rec_loss: 216.77 .. NELBO: 217.02\n",
      "****************************************************************************************************\n",
      "Epoch----->87 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 214.85 .. NELBO: 215.12\n",
      "****************************************************************************************************\n",
      "Epoch: 88 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.28 .. Rec_loss: 233.45 .. NELBO: 233.73\n",
      "Epoch: 88 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 217.73 .. NELBO: 217.97\n",
      "Epoch: 88 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 212.67 .. NELBO: 212.93\n",
      "Epoch: 88 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 218.2 .. NELBO: 218.47\n",
      "Epoch: 88 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 217.08 .. NELBO: 217.32\n",
      "****************************************************************************************************\n",
      "Epoch----->88 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 215.04 .. NELBO: 215.27\n",
      "****************************************************************************************************\n",
      "Epoch: 89 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.63 .. Rec_loss: 233.41 .. NELBO: 234.04\n",
      "Epoch: 89 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.4 .. Rec_loss: 217.92 .. NELBO: 218.32\n",
      "Epoch: 89 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.33 .. Rec_loss: 213.11 .. NELBO: 213.44\n",
      "Epoch: 89 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 218.49 .. NELBO: 218.76\n",
      "Epoch: 89 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.25 .. Rec_loss: 217.29 .. NELBO: 217.54\n",
      "****************************************************************************************************\n",
      "Epoch----->89 .. LR: 0.005 .. KL_theta: 0.25 .. Rec_loss: 215.12 .. NELBO: 215.37\n",
      "****************************************************************************************************\n",
      "Epoch: 90 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 233.07 .. NELBO: 233.39\n",
      "Epoch: 90 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 217.63 .. NELBO: 217.9\n",
      "Epoch: 90 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.29 .. Rec_loss: 212.63 .. NELBO: 212.92\n",
      "Epoch: 90 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 218.1 .. NELBO: 218.34\n",
      "Epoch: 90 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.22 .. Rec_loss: 216.93 .. NELBO: 217.15\n",
      "****************************************************************************************************\n",
      "Epoch----->90 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 214.91 .. NELBO: 215.14\n",
      "****************************************************************************************************\n",
      "Epoch: 91 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 233.47 .. NELBO: 233.64\n",
      "Epoch: 91 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 217.89 .. NELBO: 218.08\n",
      "Epoch: 91 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 212.85 .. NELBO: 213.11\n",
      "Epoch: 91 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 218.21 .. NELBO: 218.47\n",
      "Epoch: 91 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 216.95 .. NELBO: 217.18\n",
      "****************************************************************************************************\n",
      "Epoch----->91 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 214.99 .. NELBO: 215.2\n",
      "****************************************************************************************************\n",
      "Epoch: 92 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 233.28 .. NELBO: 233.77\n",
      "Epoch: 92 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.33 .. Rec_loss: 217.9 .. NELBO: 218.23\n",
      "Epoch: 92 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.35 .. Rec_loss: 213.01 .. NELBO: 213.36\n",
      "Epoch: 92 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.38 .. Rec_loss: 218.06 .. NELBO: 218.44\n",
      "Epoch: 92 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 217.03 .. NELBO: 217.35\n",
      "****************************************************************************************************\n",
      "Epoch----->92 .. LR: 0.005 .. KL_theta: 0.3 .. Rec_loss: 214.93 .. NELBO: 215.23\n",
      "****************************************************************************************************\n",
      "Epoch: 93 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 233.36 .. NELBO: 233.83\n",
      "Epoch: 93 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.41 .. Rec_loss: 217.74 .. NELBO: 218.15\n",
      "Epoch: 93 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.41 .. Rec_loss: 212.85 .. NELBO: 213.26\n",
      "Epoch: 93 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 217.92 .. NELBO: 218.39\n",
      "Epoch: 93 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.41 .. Rec_loss: 216.85 .. NELBO: 217.26\n",
      "****************************************************************************************************\n",
      "Epoch----->93 .. LR: 0.005 .. KL_theta: 0.41 .. Rec_loss: 214.8 .. NELBO: 215.21\n",
      "****************************************************************************************************\n",
      "Epoch: 94 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 234.5 .. NELBO: 234.65\n",
      "Epoch: 94 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.28 .. Rec_loss: 218.23 .. NELBO: 218.51\n",
      "Epoch: 94 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.29 .. Rec_loss: 213.24 .. NELBO: 213.53\n",
      "Epoch: 94 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.41 .. Rec_loss: 218.3 .. NELBO: 218.71\n",
      "Epoch: 94 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.38 .. Rec_loss: 217.15 .. NELBO: 217.53\n",
      "****************************************************************************************************\n",
      "Epoch----->94 .. LR: 0.005 .. KL_theta: 0.39 .. Rec_loss: 215.14 .. NELBO: 215.53\n",
      "****************************************************************************************************\n",
      "Epoch: 95 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.62 .. Rec_loss: 233.96 .. NELBO: 234.58\n",
      "Epoch: 95 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.42 .. Rec_loss: 218.17 .. NELBO: 218.59\n",
      "Epoch: 95 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.39 .. Rec_loss: 213.19 .. NELBO: 213.58\n",
      "Epoch: 95 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.56 .. Rec_loss: 218.18 .. NELBO: 218.74\n",
      "Epoch: 95 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 217.09 .. NELBO: 217.59\n",
      "****************************************************************************************************\n",
      "Epoch----->95 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 215.11 .. NELBO: 215.64\n",
      "****************************************************************************************************\n",
      "Epoch: 96 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.36 .. Rec_loss: 234.29 .. NELBO: 234.65\n",
      "Epoch: 96 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.39 .. Rec_loss: 218.02 .. NELBO: 218.41\n",
      "Epoch: 96 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.37 .. Rec_loss: 213.07 .. NELBO: 213.44\n",
      "Epoch: 96 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 217.99 .. NELBO: 218.59\n",
      "Epoch: 96 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 216.95 .. NELBO: 217.49\n",
      "****************************************************************************************************\n",
      "Epoch----->96 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 215.15 .. NELBO: 215.68\n",
      "****************************************************************************************************\n",
      "Epoch: 97 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 233.87 .. NELBO: 234.39\n",
      "Epoch: 97 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 217.8 .. NELBO: 218.3\n",
      "Epoch: 97 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.44 .. Rec_loss: 212.85 .. NELBO: 213.29\n",
      "Epoch: 97 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.69 .. Rec_loss: 217.79 .. NELBO: 218.48\n",
      "Epoch: 97 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.61 .. Rec_loss: 216.84 .. NELBO: 217.45\n",
      "****************************************************************************************************\n",
      "Epoch----->97 .. LR: 0.005 .. KL_theta: 0.58 .. Rec_loss: 215.1 .. NELBO: 215.68\n",
      "****************************************************************************************************\n",
      "Epoch: 98 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.87 .. Rec_loss: 233.47 .. NELBO: 234.34\n",
      "Epoch: 98 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.76 .. Rec_loss: 217.9 .. NELBO: 218.66\n",
      "Epoch: 98 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.64 .. Rec_loss: 212.82 .. NELBO: 213.46\n",
      "Epoch: 98 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.77 .. Rec_loss: 218.19 .. NELBO: 218.96\n",
      "Epoch: 98 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.68 .. Rec_loss: 217.13 .. NELBO: 217.81\n",
      "****************************************************************************************************\n",
      "Epoch----->98 .. LR: 0.005 .. KL_theta: 0.69 .. Rec_loss: 215.36 .. NELBO: 216.05\n",
      "****************************************************************************************************\n",
      "Epoch: 99 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.64 .. Rec_loss: 233.46 .. NELBO: 234.1\n",
      "Epoch: 99 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 218.14 .. NELBO: 218.68\n",
      "Epoch: 99 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.58 .. Rec_loss: 212.69 .. NELBO: 213.27\n",
      "Epoch: 99 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.74 .. Rec_loss: 217.98 .. NELBO: 218.72\n",
      "Epoch: 99 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.7 .. Rec_loss: 217.02 .. NELBO: 217.72\n",
      "****************************************************************************************************\n",
      "Epoch----->99 .. LR: 0.005 .. KL_theta: 0.71 .. Rec_loss: 215.22 .. NELBO: 215.93\n",
      "****************************************************************************************************\n",
      "Epoch: 100 .. batch: 20/118 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 234.43 .. NELBO: 234.93\n",
      "Epoch: 100 .. batch: 40/118 .. LR: 0.005 .. KL_theta: 0.41 .. Rec_loss: 218.64 .. NELBO: 219.05\n",
      "Epoch: 100 .. batch: 60/118 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 213.02 .. NELBO: 213.52\n",
      "Epoch: 100 .. batch: 80/118 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 218.68 .. NELBO: 219.19\n",
      "Epoch: 100 .. batch: 100/118 .. LR: 0.005 .. KL_theta: 0.56 .. Rec_loss: 217.48 .. NELBO: 218.04\n",
      "****************************************************************************************************\n",
      "Epoch----->100 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 215.89 .. NELBO: 216.4\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Train the model using default partitioning choice \n",
    "output = model.train_model(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app game like fix video make even also good get\n",
      "play game good like love thing get also make really\n",
      "game play app get fun like love try fix video\n",
      "game play like good fix get app make really love\n",
      "game play fun account app get thing good like character\n",
      "game play app get ad like money good make also\n",
      "app get good like time fix make one really use\n"
     ]
    }
   ],
   "source": [
    "for t in output['topics']:\n",
    "  print(\" \".join(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic diversity: 0.32857142857142857\n",
      "coherence CV: 0.4210142766974854\n",
      "coherence NPMI: 0.005855890315829429\n",
      "coherence UCI: 0.012371004642485176\n"
     ]
    }
   ],
   "source": [
    "topic_diversity_score = topic_diversity.score(output)\n",
    "cv_score = cv.score(output)\n",
    "npmi_score = npmi.score(output)\n",
    "uci_score = uci.score(output)\n",
    "\n",
    "print(f\"topic diversity: {topic_diversity_score}\")\n",
    "print(f\"coherence CV: {cv_score}\")\n",
    "print(f\"coherence NPMI: {npmi_score}\")\n",
    "print(f\"coherence UCI: {uci_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
