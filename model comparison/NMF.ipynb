{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import octis\n",
    "from octis.optimization.optimizer import Optimizer\n",
    "from skopt.space.space import Real, Categorical, Integer\n",
    "from octis.models.NMF import NMF\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class NMF(AbstractModel):\n",
      "\n",
      "    def __init__(\n",
      "        self, num_topics=100, chunksize=2000, passes=1, kappa=1.0,\n",
      "        minimum_probability=0.01, w_max_iter=200,\n",
      "        w_stop_condition=0.0001, h_max_iter=50, h_stop_condition=0.001,\n",
      "        eval_every=10, normalize=True, random_state=None,\n",
      "            use_partitions=True):\n",
      "        \"\"\"\n",
      "        Initialize NMF model\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        num_topics (int, optional) – Number of topics to extract.\n",
      "\n",
      "        chunksize (int, optional) – Number of documents to be used in each\n",
      "        training chunk.\n",
      "\n",
      "        passes (int, optional) – Number of full passes over the\n",
      "        training corpus. Leave at default passes=1 if your input\n",
      "        is an iterator.\n",
      "\n",
      "        kappa (float, optional) – Gradient descent step size.\n",
      "        Larger value makes the model train faster, but could\n",
      "        lead to non-convergence if set too large.\n",
      "\n",
      "        minimum_probability – If normalize is True, topics with\n",
      "        smaller probabilities are filtered out. If normalize is False,\n",
      "        topics with smaller factors are filtered out. If set to None,\n",
      "        a value of 1e-8 is used to prevent 0s.\n",
      "\n",
      "        w_max_iter (int, optional) – Maximum number of iterations to\n",
      "        train W per each batch.\n",
      "\n",
      "        w_stop_condition (float, optional) – If error difference gets less\n",
      "        than that, training of W stops for the current batch.\n",
      "\n",
      "        h_max_iter (int, optional) – Maximum number of iterations to train\n",
      "        h per each batch.\n",
      "\n",
      "        h_stop_condition (float) – If error difference gets less than that,\n",
      "        training of h stops for the current batch.\n",
      "\n",
      "        eval_every (int, optional) – Number of batches after which l2 norm\n",
      "        of (v - Wh) is computed. Decreases performance if set too low.\n",
      "\n",
      "        normalize (bool or None, optional) – Whether to normalize the result.\n",
      "\n",
      "        random_state ({np.random.RandomState, int}, optional) – Seed for\n",
      "        random generator. Needed for reproducibility.\n",
      "        \"\"\"\n",
      "        super().__init__()\n",
      "        self.hyperparameters[\"num_topics\"] = num_topics\n",
      "        self.hyperparameters[\"chunksize\"] = chunksize\n",
      "        self.hyperparameters[\"passes\"] = passes\n",
      "        self.hyperparameters[\"kappa\"] = kappa\n",
      "        self.hyperparameters[\"minimum_probability\"] = minimum_probability\n",
      "        self.hyperparameters[\"w_max_iter\"] = w_max_iter\n",
      "        self.hyperparameters[\"w_stop_condition\"] = w_stop_condition\n",
      "        self.hyperparameters[\"h_max_iter\"] = h_max_iter\n",
      "        self.hyperparameters[\"h_stop_condition\"] = h_stop_condition\n",
      "        self.hyperparameters[\"eval_every\"] = eval_every\n",
      "        self.hyperparameters[\"normalize\"] = normalize\n",
      "        self.hyperparameters[\"random_state\"] = random_state\n",
      "        self.use_partitions = use_partitions\n",
      "\n",
      "        self.id2word = None\n",
      "        self.id_corpus = None\n",
      "        self.update_with_test = False\n",
      "\n",
      "    def info(self):\n",
      "        \"\"\"\n",
      "        Returns model informations\n",
      "        \"\"\"\n",
      "        return {\n",
      "            \"citation\": citations.models_NMF,\n",
      "            \"name\": \"NMF, Non-negative Matrix Factorization\"\n",
      "        }\n",
      "\n",
      "    def hyperparameters_info(self):\n",
      "        \"\"\"\n",
      "        Returns hyperparameters informations\n",
      "        \"\"\"\n",
      "        return defaults.NMF_gensim_hyperparameters_info\n",
      "\n",
      "    def partitioning(self, use_partitions, update_with_test=False):\n",
      "        \"\"\"\n",
      "        Handle the partitioning system to use and reset the model to perform\n",
      "        new evaluations\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        use_partitions: True if train/set partitioning is needed, False\n",
      "                        otherwise\n",
      "        update_with_test: True if the model should be updated with the test set,\n",
      "                          False otherwise\n",
      "        \"\"\"\n",
      "        self.use_partitions = use_partitions\n",
      "        self.update_with_test = update_with_test\n",
      "        self.id2word = None\n",
      "        self.id_corpus = None\n",
      "\n",
      "    def train_model(self, dataset, hyperparameters=None, top_words=10):\n",
      "        \"\"\"\n",
      "        Train the model and return output\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        dataset : dataset to use to build the model\n",
      "        hyperparameters : hyperparameters to build the model\n",
      "        top_words : if greather than 0 returns the most significant words\n",
      "                 for each topic in the output\n",
      "                 Default True\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        result : dictionary with up to 3 entries,\n",
      "                 'topics', 'topic-word-matrix' and\n",
      "                 'topic-document-matrix'\n",
      "        \"\"\"\n",
      "        if hyperparameters is None:\n",
      "            hyperparameters = {}\n",
      "        if self.use_partitions:\n",
      "            partition = dataset.get_partitioned_corpus(use_validation=False)\n",
      "        else:\n",
      "            partition = [dataset.get_corpus(), []]\n",
      "\n",
      "        if self.id2word is None:\n",
      "            self.id2word = corpora.Dictionary(dataset.get_corpus())\n",
      "        if self.id_corpus is None:\n",
      "            self.id_corpus = [self.id2word.doc2bow(\n",
      "                document) for document in partition[0]]\n",
      "\n",
      "        hyperparameters[\"corpus\"] = self.id_corpus\n",
      "        hyperparameters[\"id2word\"] = self.id2word\n",
      "        self.hyperparameters.update(hyperparameters)\n",
      "\n",
      "        self.trained_model = nmf.Nmf(**self.hyperparameters)\n",
      "\n",
      "        result = {}\n",
      "\n",
      "        result[\"topic-word-matrix\"] = self.trained_model.get_topics()\n",
      "\n",
      "        if top_words > 0:\n",
      "            topics_output = []\n",
      "            for topic in result[\"topic-word-matrix\"]:\n",
      "                top_k = np.argsort(topic)[-top_words:]\n",
      "                top_k_words = list(reversed([self.id2word[i] for i in top_k]))\n",
      "                topics_output.append(top_k_words)\n",
      "            result[\"topics\"] = topics_output\n",
      "\n",
      "        result[\"topic-document-matrix\"] = self._get_topic_document_matrix()\n",
      "\n",
      "        if self.use_partitions:\n",
      "            new_corpus = [self.id2word.doc2bow(\n",
      "                document) for document in partition[1]]\n",
      "            if self.update_with_test:\n",
      "                self.trained_model.update(new_corpus)\n",
      "                self.id_corpus.extend(new_corpus)\n",
      "\n",
      "                result[\n",
      "                    \"test-topic-word-matrix\"] = self.trained_model.get_topics()\n",
      "\n",
      "                if top_words > 0:\n",
      "                    topics_output = []\n",
      "                    for topic in result[\"test-topic-word-matrix\"]:\n",
      "                        top_k = np.argsort(topic)[-top_words:]\n",
      "                        top_k_words = list(\n",
      "                            reversed([self.id2word[i] for i in top_k]))\n",
      "                        topics_output.append(top_k_words)\n",
      "                    result[\"test-topics\"] = topics_output\n",
      "\n",
      "                result[\"test-topic-document-matrix\"] = (\n",
      "                    self._get_topic_document_matrix())\n",
      "            else:\n",
      "                result[\"test-topic-document-matrix\"] = (\n",
      "                    self._get_topic_document_matrix(new_corpus))\n",
      "        return result\n",
      "\n",
      "    def _get_topics_words(self, topk):\n",
      "        \"\"\"\n",
      "        Return the most significative words for each topic.\n",
      "        \"\"\"\n",
      "        topic_terms = []\n",
      "        for i in range(self.hyperparameters[\"num_topics\"]):\n",
      "            topic_words_list = []\n",
      "            for word_tuple in self.trained_model.get_topic_terms(i, topk):\n",
      "                topic_words_list.append(self.id2word[word_tuple[0]])\n",
      "            topic_terms.append(topic_words_list)\n",
      "        return topic_terms\n",
      "\n",
      "    def _get_topic_document_matrix(self, test_corpus=None):\n",
      "        \"\"\"\n",
      "        Return the topic representation of the\n",
      "        corpus\n",
      "        \"\"\"\n",
      "        doc_topic_tuples = []\n",
      "\n",
      "        if test_corpus is None:\n",
      "            for document in self.id_corpus:\n",
      "                doc_topic_tuples.append(\n",
      "                    self.trained_model.get_document_topics(\n",
      "                        document, minimum_probability=0))\n",
      "        else:\n",
      "            for document in test_corpus:\n",
      "                doc_topic_tuples.append(\n",
      "                    self.trained_model.get_document_topics(\n",
      "                        document, minimum_probability=0))\n",
      "        topic_document = np.zeros((\n",
      "            self.hyperparameters[\"num_topics\"],\n",
      "            len(doc_topic_tuples)))\n",
      "\n",
      "        for ndoc in range(len(doc_topic_tuples)):\n",
      "            document = doc_topic_tuples[ndoc]\n",
      "            for topic_tuple in document:\n",
      "                topic_document[topic_tuple[0]][ndoc] = topic_tuple[1]\n",
      "        return topic_document\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(octis.models.NMF.NMF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.load_custom_dataset_from_folder(\"content/corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = Coherence(texts=dataset.get_corpus(), measure='c_v')\n",
    "umass = Coherence(texts=dataset.get_corpus(), measure='u_mass')\n",
    "uci = Coherence(texts=dataset.get_corpus(), measure='c_uci')\n",
    "npmi = Coherence(texts=dataset.get_corpus())\n",
    "topic_diversity = TopicDiversity(topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "model = NMF(num_topics=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using default partitioning choice \n",
    "output = model.train_model(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app fix make great please cash issue bug good keep\n",
      "get back post go try game fix notification also message\n",
      "like game play make really good one will message video\n",
      "update work time app send message account issue keep even\n",
      "video app watch ad work post play youtube edit go\n",
      "use app ad get even good see update go bad\n",
      "app try use feature time want phone work also give\n"
     ]
    }
   ],
   "source": [
    "for t in output['topics']:\n",
    "  print(\" \".join(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic diversity: 0.6\n",
      "coherence CV: 0.3883118638039832\n",
      "coherence NPMI: -0.0010630785564898792\n",
      "coherence UCI: -0.0564292041172054\n"
     ]
    }
   ],
   "source": [
    "topic_diversity_score = topic_diversity.score(output)\n",
    "cv_score = cv.score(output)\n",
    "npmi_score = npmi.score(output)\n",
    "uci_score = uci.score(output)\n",
    "\n",
    "print(f\"topic diversity: {topic_diversity_score}\")\n",
    "print(f\"coherence CV: {cv_score}\")\n",
    "print(f\"coherence NPMI: {npmi_score}\")\n",
    "print(f\"coherence UCI: {uci_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
